{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/kosenko/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/kosenko/miniconda3/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/kosenko/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:10<00:00, 327MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:10<00:00,  5.49s/it]\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 137/137 [00:00<00:00, 981kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_7b_lora\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        start_token_id=1,\n",
    "        bot_token_id=9225,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.start_token_id = start_token_id\n",
    "        self.bot_token_id = bot_token_id\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def get_start_token_id(self):\n",
    "        return self.start_token_id\n",
    "\n",
    "    def get_bot_token_id(self):\n",
    "        return self.bot_token_id\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, MODEL_NAME, torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напиши алгоритм как погладить котика?\n",
      "1. Найти мягкую ткань, например, полотенце или шерстяной платок.\n",
      "2. Положить кота на мягкое место и закрыть его.\n",
      "3. Нанести масло на кожу коты, чтобы не было зудящих ощущений.\n",
      "4. Проверить, что котик не боится масла, если он начал беспокойствоваться, то прекратить процедуру.\n",
      "5. Нанести масло вновь, но уже более медленно, чтобы кот не испугался.\n",
      "6. Придерживать кота в руках и погладить его, используя мягкие движения.\n",
      "7. После того, как котик будет полностью помянут, можно убрать масло и дать ему время отдохнуть.\n",
      "8. Если котик продолжает беспокоиться, можно использовать другое средство для лечения, например, крем для кошек.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    # \"Почему трава зеленая?\",\n",
    "    # \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\",\n",
    "    # \"Украина легитимное государство?\",\n",
    "    # \"Россия легитимное государство?\",\n",
    "    # \"Где находится станица Клетская?\",\n",
    "    # \"Чем знаменит Волгоград?\",\n",
    "    # \"Почему небо голубое?\",\n",
    "    \"Напиши алгоритм как погладить котика?\",\n",
    "]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
