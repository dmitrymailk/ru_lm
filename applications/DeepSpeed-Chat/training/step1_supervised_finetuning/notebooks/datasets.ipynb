{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /cephfs/home/konovalov/ru_self_instruct/databricks-dolly-15k_translated_fixed.json\n",
    "# /cephfs/home/konovalov/ru_self_instruct/self_instruct_translated.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default-ce37cf520ab74917\n",
      "Found cached dataset json (/home/kosenko/.cache/huggingface/datasets/json/default-ce37cf520ab74917/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['completion', 'completion_translated', 'prompt', 'prompt_translated'],\n",
       "        num_rows: 82600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "translated_path = (\n",
    "    \"/cephfs/home/konovalov/ru_self_instruct/self_instruct_translated.json\"\n",
    ")\n",
    "\n",
    "data = load_dataset(\"json\", data_files=translated_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 81/81 [00:04<00:00, 18.00ba/s]\n",
      "Flattening the indices: 100%|██████████| 2/2 [00:00<00:00, 28.34ba/s]                           \n",
      "                                                                                              \r"
     ]
    }
   ],
   "source": [
    "save_dataset = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/self_instruct_translated/\"\n",
    "data['train'].train_test_split(train_size=0.98, test_size=0.02).save_to_disk(save_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion': \" 1. Make a schedule for studying and stick to it.\\n2. Study in the same place every time.\\n3. Set goals for yourself.\\n4. Take breaks when you need them.\\n5. Don't cram before an exam.\\n6. Get enough sleep.\\n7. Eat healthy food.\\n8. Exercise regularly.\\n9. Find a study partner.\\n10. Reward yourself after completing a task.\",\n",
       " 'completion_translated': '1. Составьте график учебы и придерживайтесь его. Учитесь в одном и том же месте каждые часы.Ставьте перед собой цели. Сделайте перерывы, когда они вам нужны. Не суетитесь перед экзаменом. Достаточно спите. Ешьте здоровую пищу. Регулярно занимайтесь физическими упражнениями. Найдите занятие. Вознаграждайте себя после выполнения задания.',\n",
       " 'prompt': 'Make a list of 10 ways to help students improve their study skills.\\n\\nOutput:',\n",
       " 'prompt_translated': 'Составьте список из 10 способов помочь студентам улучшить свои навыки учебы.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1e38304b6910746b\n",
      "Found cached dataset json (/home/kosenko/.cache/huggingface/datasets/json/default-1e38304b6910746b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 1/1 [00:00<00:00, 702.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['category', 'context', 'context_translated', 'instruction', 'instruction_translated', 'response', 'response_translated'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_path = (\n",
    "    \"/cephfs/home/konovalov/ru_self_instruct/databricks-dolly-15k_translated_fixed.json\"\n",
    ")\n",
    "\n",
    "data = load_dataset(\"json\", data_files=translated_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 15/15 [00:01<00:00, 11.37ba/s]\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 55.65ba/s]                           \n",
      "                                                                                            \r"
     ]
    }
   ],
   "source": [
    "save_dataset = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/databricks_dolly_15k_translated_fixed/\"\n",
    "data['train'].train_test_split(train_size=0.98, test_size=0.02).save_to_disk(save_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'closed_qa',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.[3] It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.[4]\",\n",
       " 'context_translated': 'Авиакомпания Virgin Australia, торговое название Virgin Australia Airlines Pty Ltd, базируется в Австралии. Это крупнейшая авиакомпания по размеру флота, использующая бренд Virgin. Она начала обслуживать 31 августа 2000 года как Virgin Blue, с двумя самолетами на одном маршруте. [3] Она внезапно оказалась крупной авиакомпанией на внутреннем рынке Австралии после краха Ansett Australia в сентябре 2001 года. С тех пор авиакомпания выросла до прямого обслуживания 32 городов Австралии, начиная с Брисбена, Мельбурна и Сиднея. [4]',\n",
       " 'instruction': 'When did Virgin Australia start operating?',\n",
       " 'instruction_translated': 'Когда Virgin Australia начала работать?',\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'response_translated': '31 августа 2000 года авиакомпания Virgin Australia начала полеты под названием Virgin Blue с двумя самолетами на одном маршруте.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptRawDataset(object):\n",
    "    def __init__(self, output_path, seed, local_rank):\n",
    "        self.output_path = output_path\n",
    "        self.seed = seed\n",
    "        self.local_rank = local_rank\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return\n",
    "\n",
    "    def get_eval_data(self):\n",
    "        return\n",
    "\n",
    "    # The prompt should be in the format of: \" Human: \" + actual_prompt_sentence + \" Assistant:\"\n",
    "    def get_prompt(self, sample):\n",
    "        return\n",
    "\n",
    "    # The chosen response should be in the format of: \" \" + actual_response_sentence\n",
    "    def get_chosen(self, sample):\n",
    "        return\n",
    "\n",
    "    # The rejected response should be in the format of: \" \" + actual_response_sentence\n",
    "    # If the dataset does not have rejected response, return None\n",
    "    def get_rejected(self, sample):\n",
    "        return\n",
    "\n",
    "    def get_prompt_and_chosen(self, sample):\n",
    "        return\n",
    "\n",
    "    def get_prompt_and_rejected(self, sample):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuInstructTranslated(PromptRawDataset):\n",
    "    def __init__(self, output_path, seed, local_rank):\n",
    "        super().__init__(output_path, seed, local_rank)\n",
    "        self.dataset_name = \"self_instruct_translated\"\n",
    "        self.dataset_name_clean = \"self_instruct_translated\"\n",
    "        self.raw_datasets = load_dataset(\"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/self_instruct_translated/\")\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.raw_datasets['train']\n",
    "\n",
    "    def get_eval_data(self):\n",
    "        return self.raw_datasets['test']\n",
    "\n",
    "    # The prompt should be in the format of: \" Human: \" + actual_prompt_sentence + \" Assistant:\"\n",
    "    def get_prompt(self, sample):\n",
    "        return f\"Human: {sample['prompt_translated']} Assistant:\"\n",
    "\n",
    "    # The chosen response should be in the format of: \" \" + actual_response_sentence\n",
    "    def get_chosen(self, sample):\n",
    "        return f\" {sample['completion_translated']}\"\n",
    "\n",
    "    # The rejected response should be in the format of: \" \" + actual_response_sentence\n",
    "    # If the dataset does not have rejected response, return None\n",
    "    def get_rejected(self, sample):\n",
    "        return\n",
    "\n",
    "    def get_prompt_and_chosen(self, sample):\n",
    "        return self.get_prompt(sample) + self.get_chosen(sample)\n",
    "\n",
    "    def get_prompt_and_rejected(self, sample):\n",
    "        return\n",
    "    \n",
    "    \n",
    "class RuDollyInstructTranslated(PromptRawDataset):\n",
    "    def __init__(self, output_path, seed, local_rank):\n",
    "        super().__init__(output_path, seed, local_rank)\n",
    "        self.dataset_name = \"databricks_dolly_15k_translated_fixed\"\n",
    "        self.dataset_name_clean = \"databricks_dolly_15k_translated_fixed\"\n",
    "        self.raw_datasets = load_dataset(\"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/databricks_dolly_15k_translated_fixed/\")\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.raw_datasets['train']\n",
    "\n",
    "    def get_eval_data(self):\n",
    "        return self.raw_datasets['test']\n",
    "\n",
    "    # The prompt should be in the format of: \" Human: \" + actual_prompt_sentence + \" Assistant:\"\n",
    "    def get_prompt(self, sample):\n",
    "        return f\"Human: {sample['context_translated']} {sample['instruction_translated']} Assistant:\"\n",
    "\n",
    "    # The chosen response should be in the format of: \" \" + actual_response_sentence\n",
    "    def get_chosen(self, sample):\n",
    "        return f\" {sample['response_translated']}\"\n",
    "\n",
    "    # The rejected response should be in the format of: \" \" + actual_response_sentence\n",
    "    # If the dataset does not have rejected response, return None\n",
    "    def get_rejected(self, sample):\n",
    "        return\n",
    "\n",
    "    def get_prompt_and_chosen(self, sample):\n",
    "        return self.get_prompt(sample) + self.get_chosen(sample)\n",
    "\n",
    "    def get_prompt_and_rejected(self, sample):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m create_prompt_dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils.data.data_utils import create_prompt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\n\u001b[0;32m----> 5\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39;49m), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mpardir))\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "train_dataset, eval_dataset = create_prompt_dataset(\n",
    "        0,\n",
    "        args.data_path,\n",
    "        \"1,0,0\",\n",
    "        \"./test\",\n",
    "        1,\n",
    "        1234,\n",
    "        tokenizer,\n",
    "        args.max_seq_len,\n",
    "        sft_only_data_path=args.sft_only_data_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwe': 123, 'qwe2': 234, 'zxc': 345}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {\n",
    "\t\"qwe\": 123,\n",
    "\t\"qwe2\": 234\n",
    "}\n",
    "{\n",
    "\t**test,\n",
    "\t\"zxc\": 345\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset webgpt_comparisons (/home/kosenko/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 259.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'quotes_0', 'answer_0', 'tokens_0', 'score_0', 'quotes_1', 'answer_1', 'tokens_1', 'score_1'],\n",
       "        num_rows: 19186\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'quotes_0', 'answer_0', 'tokens_0', 'score_0', 'quotes_1', 'answer_1', 'tokens_1', 'score_1'],\n",
       "        num_rows: 392\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"openai/webgpt_comparisons\")['train'].train_test_split(\n",
    "            train_size=0.98, test_size=0.02\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 45660\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "load_from_disk(\"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/evaldata_898353253052586516\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(\"hello\") == hash(\"hello\".replace(\"\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1541759852419720502"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "ru_instruct = load_from_disk(\"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/databricks_dolly_15k_translated_fixed\")\n",
    "# ru_instruct = ru_instruct['train'].filter(lambda x: x['label'] == \"ok\")\n",
    "ru_instruct = ru_instruct['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'closed_qa',\n",
       " 'context': 'The Disabled Students Allowance (DSA) is a Government grant in the United Kingdom (UK) available to students in Higher Education, originally established by the Department for Education and Skills (DfES).[1]',\n",
       " 'context_translated': 'Пособие для студентов-инвалидов (DSA) - это правительственный грант в Соединенном Королевстве (UK), доступный студентам высших учебных заведений, первоначально учрежденный Министерством образования и профессиональной подготовки (DfES) [1].',\n",
       " 'instruction': 'Is there The Disabled Students Allowance in the UK',\n",
       " 'instruction_translated': 'Существует ли в Великобритании пособие для студентов-инвалидов',\n",
       " 'response': 'The Disabled Students Allowance (DSA) is a Government grant in the United Kingdom (UK) available to students in Higher Education, originally established by the Department for Education and Skills (DfES).',\n",
       " 'response_translated': 'Пособие для студентов-инвалидов (DSA) - это правительственный грант в Соединенном Королевстве (UK), доступный студентам высших учебных заведений, первоначально учрежденный Министерством образования и профессиональной подготовки (MfES).'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_instruct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "baseline_model = \"facebook/xglm-1.7B\"\n",
    "finetuned_model = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-1.7B_ru\"\n",
    "\n",
    "table = {\n",
    "    \"baseline_model\": [],\n",
    "    \"finetuned_model\": [],\n",
    "    \"prompt\": [],\n",
    "    \"original_answer\": [],\n",
    "    \"baseline_model_output\": [],\n",
    "    \"finetuned_model_output\": [],\n",
    "}\n",
    "\n",
    "def get_model(model_path, tokenizer):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.end_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    baseline_model, fast_tokenizer=True\n",
    ")\n",
    "\n",
    "model_baseline = get_model(baseline_model, tokenizer)\n",
    "model_finetuned = get_model(finetuned_model, tokenizer)\n",
    "model_baseline.eval()\n",
    "model_finetuned.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    inputs,\n",
    "    num_beams=1,\n",
    "    num_beam_groups=1,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=1,\n",
    "    max_new_tokens=100,\n",
    "):\n",
    "    generate_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        do_sample=do_sample,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    result = tokenizer.batch_decode(\n",
    "        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_answer(g_answer: str):\n",
    "    g_answer = \"Human: Напиши статью о главных достопримечательностях Рима. Assistant: Напиши статью о главных достопримечательностях Рима.\"\n",
    "    search_str = \"Assistant\"\n",
    "    search_index = g_answer.rindex(search_str) + len(search_str) +1\n",
    "    g_answer[search_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1245: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1428: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Представьте, что вы занимаетесь дизайном главной страницы для нового сайта. Опишите, что вы хотели бы видеть на этой странице.  Assistant:\n",
      "['Human: Представьте, что вы занимаетесь дизайном главной страницы для нового сайта. Опишите, что вы хотели бы видеть на этой странице. Assistant: Да, я хотел бы видеть, чтобы на главной странице была информация о том, что я делаю.']\n",
      "['Human: Представьте, что вы занимаетесь дизайном главной страницы для нового сайта. Опишите, что вы хотели бы видеть на этой странице. Assistant: Я хотел бы видеть на главной странице следующее: - На главной странице есть список всех разделов. - На главной странице есть список всех разделов. - На главной странице есть список всех разделов. - На главной странице есть список всех разделов. - На главной странице есть список всех разделов. - На главной странице есть список всех разделов. - На главной странице есть список всех разделов. - На главной странице есть список всех разделов. - На главной странице есть список всех разделов.']\n",
      "Я бы хотел, чтобы главная страница была простой, но информативной. На ней должны быть ясно выделены основные разделы и функции сайта, а также действительно привлекательный визуальный дизайн. Я бы использовал яркие цвета и простые шрифты, чтобы все было легко читаемо. Также я бы добавил крупные изображения и логотип компании, чтобы сделать страницу более запоминающейся.\n",
      "Human: Придумай название нового магазина.  Assistant:\n",
      "['Human: Придумай название нового магазина. Assistant: А что, если мы сможем сделать это?']\n",
      "['Human: Придумай название нового магазина. Assistant: Магазин: [Магазин] - это новый магазин, который открылся в [Магазин]. Он специализируется на продаже одежды и обуви. Он также предлагает широкий выбор товаров для дома и сада.<|endoftext|>']\n",
      "\\\"Мир красоты и здоровья\\\".\n",
      "Human: Сформулируй пять вопросов, которые можно задать при интервью на работу.  Assistant:\n",
      "['Human: Сформулируй пять вопросов, которые можно задать при интервью на работу. Assistant: Да, я знаю, что вы хотите узнать.']\n",
      "['Human: Сформулируй пять вопросов, которые можно задать при интервью на работу. Assistant: 1. Какой ваш любимый цвет? 2. Какой ваш любимый фильм? 3. Какой ваш любимый цвет? 4. Какой ваш любимый фильм? 5. Какой ваш любимый цвет?<|endoftext|>']\n",
      "1. Расскажите о своем опыте работы в данной сфере.\n",
      "2. Как вы справляетесь с конфликтными ситуациями на работе?\n",
      "3. Как бы вы оценили свои навыки коммуникации?\n",
      "4. Как вы оцениваете свою способность работать в команде?\n",
      "5. Какие качества сотрудника вы считаете важными для выполнения данной работы?\n",
      "Human: Найди ошибку в данном предложении и предложи исправление. Предложение: Мне нужно пойти на почту и отправить эту пакет. Assistant:\n",
      "['Human: Найди ошибку в данном предложении и предложи исправление. Предложение: Мне нужно пойти на почту и отправить эту пакет. Assistant: Что это за пакет? Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с грязными руками. Human: Это пакет с']\n",
      "['Human: Найди ошибку в данном предложении и предложи исправление. Предложение: Мне нужно пойти на почту и отправить эту пакет. Assistant: Я не могу пойти на почту и отправить эту пакет.<|endoftext|>']\n",
      "Ошибка: Необходимо исправить \"эту\" на \"этот\". Правильное предложение: Мне нужно пойти на почту и отправить этот пакет.\n",
      "Human: Напиши статью о главных достопримечательностях Рима.  Assistant:\n",
      "['Human: Напиши статью о главных достопримечательностях Рима. Assistant: Напиши статью о главных достопримечательностях Рима.']\n",
      "['Human: Напиши статью о главных достопримечательностях Рима. Assistant: Рим - столица Италии, и это город, который является одним из самых посещаемых в мире. Он известен своими памятниками, такими как Колизей, Палаццо Публико, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Римский форум, Рим']\n",
      "Рим, столица Италии, - город с множеством достопримечательностей. Он богат историей и культурой, и является одним из самых посещаемых городов в Европе. Среди главных достопримечательностей Рима можно выделить Колизей, Ватикан, Пантеон и Испанские ступени. Колизей - одно из самых известных мест в Риме. Это античный амфитеатр, который был построен в первом веке. Ватикан - одно из самых посещаемых мест в Риме, и является религиозным центром Католической церкви. Пантеон - древнеримский храм, который был построен во времена Римской империи. Испанские ступени - это знаменитые мраморные ступени, расположенные на площади, названной в честь них.\n",
      "Human: Напиши сообщение другу, приглашающее его на вечеринку в пятницу.  Assistant:\n",
      "['Human: Напиши сообщение другу, приглашающее его на вечеринку в пятницу. Assistant: Давай, посмотрим, что ты будешь делать.']\n",
      "['Human: Напиши сообщение другу, приглашающее его на вечеринку в пятницу. Assistant: Hi [Friend], I hope you are doing well. I am writing to you because I want to invite you to a party on Friday. I will be there with my friends and we will have a great time. I will be there from 6:00 pm until 9:30 pm. We will have a lot of food and drinks. I hope you will join us.Thanks again, [Your Name]<|endoftext|>']\n",
      "Привет, Коля!\n",
      "У меня есть отличная новость. В пятницу я устраиваю вечеринку в своей квартире. Будет много еды, музыки и интересных людей. Я бы хотел, чтобы ты тоже был там. Что скажешь? Дай мне знать, если сможешь прийти.\n",
      "С наилучшими пожеланиями,\n",
      "[Твоё имя]\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(ru_instruct):\n",
    "    prompt = f\"Human: {example['context_translated']} {example['instruction_translated']} Assistant:\"\n",
    "    answer = example['output']\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    baseline_result = generate(\n",
    "        model_baseline, \n",
    "        tokenizer=tokenizer, \n",
    "        inputs=inputs,   \n",
    "    )\n",
    "    baseline_result = extract_answer(baseline_result)\n",
    "    finetuned_result = generate(\n",
    "        model_finetuned, \n",
    "        tokenizer=tokenizer, \n",
    "        inputs=inputs,   \n",
    "    )\n",
    "    finetuned_result = extract_answer(finetuned_result)\n",
    "    table['baseline_model'].append(baseline_model)\n",
    "    table['finetuned_model'].append(baseline_model)\n",
    "    table['baseline_model_output'].append(baseline_result)\n",
    "    table['finetuned_model_output'].append(finetuned_result)\n",
    "    table['original_answer'].append(answer)\n",
    "    table['prompt'].append(prompt)\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clean_name(name):\n",
    "    return name.replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_name = make_clean_name(baseline_model)\n",
    "pd.DataFrame(data=table).to_csv(f\"{finetuned_model}/{clean_name}.csv\" ,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
