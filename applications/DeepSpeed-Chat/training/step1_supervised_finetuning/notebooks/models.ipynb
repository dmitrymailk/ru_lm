{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-1.7B\")\n",
    "# model = AutoModel.from_pretrained(\"facebook/xglm-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "num = 256008 + 56\n",
    "for i in range(1, 65):\n",
    "    if (i + num) % 64 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v10/epoch=6_step=61712\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"xglm-4.5B_ru_v10_epoch_6_step_61712\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.k_proj.bias\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.v_proj.bias\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.q_proj.bias\n",
      "model.layers.0.self_attn.out_proj.weight\n",
      "model.layers.0.self_attn.out_proj.bias\n",
      "model.layers.0.self_attn_layer_norm.weight\n",
      "model.layers.0.self_attn_layer_norm.bias\n",
      "model.layers.0.fc1.weight\n",
      "model.layers.0.fc1.bias\n",
      "model.layers.0.fc2.weight\n",
      "model.layers.0.fc2.bias\n",
      "model.layers.0.final_layer_norm.weight\n",
      "model.layers.0.final_layer_norm.bias\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.k_proj.bias\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.v_proj.bias\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.q_proj.bias\n",
      "model.layers.1.self_attn.out_proj.weight\n",
      "model.layers.1.self_attn.out_proj.bias\n",
      "model.layers.1.self_attn_layer_norm.weight\n",
      "model.layers.1.self_attn_layer_norm.bias\n",
      "model.layers.1.fc1.weight\n",
      "model.layers.1.fc1.bias\n",
      "model.layers.1.fc2.weight\n",
      "model.layers.1.fc2.bias\n",
      "model.layers.1.final_layer_norm.weight\n",
      "model.layers.1.final_layer_norm.bias\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.k_proj.bias\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.v_proj.bias\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.q_proj.bias\n",
      "model.layers.2.self_attn.out_proj.weight\n",
      "model.layers.2.self_attn.out_proj.bias\n",
      "model.layers.2.self_attn_layer_norm.weight\n",
      "model.layers.2.self_attn_layer_norm.bias\n",
      "model.layers.2.fc1.weight\n",
      "model.layers.2.fc1.bias\n",
      "model.layers.2.fc2.weight\n",
      "model.layers.2.fc2.bias\n",
      "model.layers.2.final_layer_norm.weight\n",
      "model.layers.2.final_layer_norm.bias\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.k_proj.bias\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.v_proj.bias\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.q_proj.bias\n",
      "model.layers.3.self_attn.out_proj.weight\n",
      "model.layers.3.self_attn.out_proj.bias\n",
      "model.layers.3.self_attn_layer_norm.weight\n",
      "model.layers.3.self_attn_layer_norm.bias\n",
      "model.layers.3.fc1.weight\n",
      "model.layers.3.fc1.bias\n",
      "model.layers.3.fc2.weight\n",
      "model.layers.3.fc2.bias\n",
      "model.layers.3.final_layer_norm.weight\n",
      "model.layers.3.final_layer_norm.bias\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.k_proj.bias\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.v_proj.bias\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.q_proj.bias\n",
      "model.layers.4.self_attn.out_proj.weight\n",
      "model.layers.4.self_attn.out_proj.bias\n",
      "model.layers.4.self_attn_layer_norm.weight\n",
      "model.layers.4.self_attn_layer_norm.bias\n",
      "model.layers.4.fc1.weight\n",
      "model.layers.4.fc1.bias\n",
      "model.layers.4.fc2.weight\n",
      "model.layers.4.fc2.bias\n",
      "model.layers.4.final_layer_norm.weight\n",
      "model.layers.4.final_layer_norm.bias\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.k_proj.bias\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.v_proj.bias\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.q_proj.bias\n",
      "model.layers.5.self_attn.out_proj.weight\n",
      "model.layers.5.self_attn.out_proj.bias\n",
      "model.layers.5.self_attn_layer_norm.weight\n",
      "model.layers.5.self_attn_layer_norm.bias\n",
      "model.layers.5.fc1.weight\n",
      "model.layers.5.fc1.bias\n",
      "model.layers.5.fc2.weight\n",
      "model.layers.5.fc2.bias\n",
      "model.layers.5.final_layer_norm.weight\n",
      "model.layers.5.final_layer_norm.bias\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.k_proj.bias\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.v_proj.bias\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.q_proj.bias\n",
      "model.layers.6.self_attn.out_proj.weight\n",
      "model.layers.6.self_attn.out_proj.bias\n",
      "model.layers.6.self_attn_layer_norm.weight\n",
      "model.layers.6.self_attn_layer_norm.bias\n",
      "model.layers.6.fc1.weight\n",
      "model.layers.6.fc1.bias\n",
      "model.layers.6.fc2.weight\n",
      "model.layers.6.fc2.bias\n",
      "model.layers.6.final_layer_norm.weight\n",
      "model.layers.6.final_layer_norm.bias\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.k_proj.bias\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.v_proj.bias\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.q_proj.bias\n",
      "model.layers.7.self_attn.out_proj.weight\n",
      "model.layers.7.self_attn.out_proj.bias\n",
      "model.layers.7.self_attn_layer_norm.weight\n",
      "model.layers.7.self_attn_layer_norm.bias\n",
      "model.layers.7.fc1.weight\n",
      "model.layers.7.fc1.bias\n",
      "model.layers.7.fc2.weight\n",
      "model.layers.7.fc2.bias\n",
      "model.layers.7.final_layer_norm.weight\n",
      "model.layers.7.final_layer_norm.bias\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.k_proj.bias\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.v_proj.bias\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.q_proj.bias\n",
      "model.layers.8.self_attn.out_proj.weight\n",
      "model.layers.8.self_attn.out_proj.bias\n",
      "model.layers.8.self_attn_layer_norm.weight\n",
      "model.layers.8.self_attn_layer_norm.bias\n",
      "model.layers.8.fc1.weight\n",
      "model.layers.8.fc1.bias\n",
      "model.layers.8.fc2.weight\n",
      "model.layers.8.fc2.bias\n",
      "model.layers.8.final_layer_norm.weight\n",
      "model.layers.8.final_layer_norm.bias\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.k_proj.bias\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.v_proj.bias\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.q_proj.bias\n",
      "model.layers.9.self_attn.out_proj.weight\n",
      "model.layers.9.self_attn.out_proj.bias\n",
      "model.layers.9.self_attn_layer_norm.weight\n",
      "model.layers.9.self_attn_layer_norm.bias\n",
      "model.layers.9.fc1.weight\n",
      "model.layers.9.fc1.bias\n",
      "model.layers.9.fc2.weight\n",
      "model.layers.9.fc2.bias\n",
      "model.layers.9.final_layer_norm.weight\n",
      "model.layers.9.final_layer_norm.bias\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.k_proj.bias\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.v_proj.bias\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.q_proj.bias\n",
      "model.layers.10.self_attn.out_proj.weight\n",
      "model.layers.10.self_attn.out_proj.bias\n",
      "model.layers.10.self_attn_layer_norm.weight\n",
      "model.layers.10.self_attn_layer_norm.bias\n",
      "model.layers.10.fc1.weight\n",
      "model.layers.10.fc1.bias\n",
      "model.layers.10.fc2.weight\n",
      "model.layers.10.fc2.bias\n",
      "model.layers.10.final_layer_norm.weight\n",
      "model.layers.10.final_layer_norm.bias\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.k_proj.bias\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.v_proj.bias\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.q_proj.bias\n",
      "model.layers.11.self_attn.out_proj.weight\n",
      "model.layers.11.self_attn.out_proj.bias\n",
      "model.layers.11.self_attn_layer_norm.weight\n",
      "model.layers.11.self_attn_layer_norm.bias\n",
      "model.layers.11.fc1.weight\n",
      "model.layers.11.fc1.bias\n",
      "model.layers.11.fc2.weight\n",
      "model.layers.11.fc2.bias\n",
      "model.layers.11.final_layer_norm.weight\n",
      "model.layers.11.final_layer_norm.bias\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.k_proj.bias\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.v_proj.bias\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.q_proj.bias\n",
      "model.layers.12.self_attn.out_proj.weight\n",
      "model.layers.12.self_attn.out_proj.bias\n",
      "model.layers.12.self_attn_layer_norm.weight\n",
      "model.layers.12.self_attn_layer_norm.bias\n",
      "model.layers.12.fc1.weight\n",
      "model.layers.12.fc1.bias\n",
      "model.layers.12.fc2.weight\n",
      "model.layers.12.fc2.bias\n",
      "model.layers.12.final_layer_norm.weight\n",
      "model.layers.12.final_layer_norm.bias\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.k_proj.bias\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.v_proj.bias\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.q_proj.bias\n",
      "model.layers.13.self_attn.out_proj.weight\n",
      "model.layers.13.self_attn.out_proj.bias\n",
      "model.layers.13.self_attn_layer_norm.weight\n",
      "model.layers.13.self_attn_layer_norm.bias\n",
      "model.layers.13.fc1.weight\n",
      "model.layers.13.fc1.bias\n",
      "model.layers.13.fc2.weight\n",
      "model.layers.13.fc2.bias\n",
      "model.layers.13.final_layer_norm.weight\n",
      "model.layers.13.final_layer_norm.bias\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.k_proj.bias\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.v_proj.bias\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.q_proj.bias\n",
      "model.layers.14.self_attn.out_proj.weight\n",
      "model.layers.14.self_attn.out_proj.bias\n",
      "model.layers.14.self_attn_layer_norm.weight\n",
      "model.layers.14.self_attn_layer_norm.bias\n",
      "model.layers.14.fc1.weight\n",
      "model.layers.14.fc1.bias\n",
      "model.layers.14.fc2.weight\n",
      "model.layers.14.fc2.bias\n",
      "model.layers.14.final_layer_norm.weight\n",
      "model.layers.14.final_layer_norm.bias\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.k_proj.bias\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.v_proj.bias\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.q_proj.bias\n",
      "model.layers.15.self_attn.out_proj.weight\n",
      "model.layers.15.self_attn.out_proj.bias\n",
      "model.layers.15.self_attn_layer_norm.weight\n",
      "model.layers.15.self_attn_layer_norm.bias\n",
      "model.layers.15.fc1.weight\n",
      "model.layers.15.fc1.bias\n",
      "model.layers.15.fc2.weight\n",
      "model.layers.15.fc2.bias\n",
      "model.layers.15.final_layer_norm.weight\n",
      "model.layers.15.final_layer_norm.bias\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.k_proj.bias\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.v_proj.bias\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.q_proj.bias\n",
      "model.layers.16.self_attn.out_proj.weight\n",
      "model.layers.16.self_attn.out_proj.bias\n",
      "model.layers.16.self_attn_layer_norm.weight\n",
      "model.layers.16.self_attn_layer_norm.bias\n",
      "model.layers.16.fc1.weight\n",
      "model.layers.16.fc1.bias\n",
      "model.layers.16.fc2.weight\n",
      "model.layers.16.fc2.bias\n",
      "model.layers.16.final_layer_norm.weight\n",
      "model.layers.16.final_layer_norm.bias\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.k_proj.bias\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.v_proj.bias\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.q_proj.bias\n",
      "model.layers.17.self_attn.out_proj.weight\n",
      "model.layers.17.self_attn.out_proj.bias\n",
      "model.layers.17.self_attn_layer_norm.weight\n",
      "model.layers.17.self_attn_layer_norm.bias\n",
      "model.layers.17.fc1.weight\n",
      "model.layers.17.fc1.bias\n",
      "model.layers.17.fc2.weight\n",
      "model.layers.17.fc2.bias\n",
      "model.layers.17.final_layer_norm.weight\n",
      "model.layers.17.final_layer_norm.bias\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.k_proj.bias\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.v_proj.bias\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.q_proj.bias\n",
      "model.layers.18.self_attn.out_proj.weight\n",
      "model.layers.18.self_attn.out_proj.bias\n",
      "model.layers.18.self_attn_layer_norm.weight\n",
      "model.layers.18.self_attn_layer_norm.bias\n",
      "model.layers.18.fc1.weight\n",
      "model.layers.18.fc1.bias\n",
      "model.layers.18.fc2.weight\n",
      "model.layers.18.fc2.bias\n",
      "model.layers.18.final_layer_norm.weight\n",
      "model.layers.18.final_layer_norm.bias\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.k_proj.bias\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.v_proj.bias\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.q_proj.bias\n",
      "model.layers.19.self_attn.out_proj.weight\n",
      "model.layers.19.self_attn.out_proj.bias\n",
      "model.layers.19.self_attn_layer_norm.weight\n",
      "model.layers.19.self_attn_layer_norm.bias\n",
      "model.layers.19.fc1.weight\n",
      "model.layers.19.fc1.bias\n",
      "model.layers.19.fc2.weight\n",
      "model.layers.19.fc2.bias\n",
      "model.layers.19.final_layer_norm.weight\n",
      "model.layers.19.final_layer_norm.bias\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.k_proj.bias\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.v_proj.bias\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.q_proj.bias\n",
      "model.layers.20.self_attn.out_proj.weight\n",
      "model.layers.20.self_attn.out_proj.bias\n",
      "model.layers.20.self_attn_layer_norm.weight\n",
      "model.layers.20.self_attn_layer_norm.bias\n",
      "model.layers.20.fc1.weight\n",
      "model.layers.20.fc1.bias\n",
      "model.layers.20.fc2.weight\n",
      "model.layers.20.fc2.bias\n",
      "model.layers.20.final_layer_norm.weight\n",
      "model.layers.20.final_layer_norm.bias\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.k_proj.bias\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.v_proj.bias\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.q_proj.bias\n",
      "model.layers.21.self_attn.out_proj.weight\n",
      "model.layers.21.self_attn.out_proj.bias\n",
      "model.layers.21.self_attn_layer_norm.weight\n",
      "model.layers.21.self_attn_layer_norm.bias\n",
      "model.layers.21.fc1.weight\n",
      "model.layers.21.fc1.bias\n",
      "model.layers.21.fc2.weight\n",
      "model.layers.21.fc2.bias\n",
      "model.layers.21.final_layer_norm.weight\n",
      "model.layers.21.final_layer_norm.bias\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.k_proj.bias\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.v_proj.bias\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.q_proj.bias\n",
      "model.layers.22.self_attn.out_proj.weight\n",
      "model.layers.22.self_attn.out_proj.bias\n",
      "model.layers.22.self_attn_layer_norm.weight\n",
      "model.layers.22.self_attn_layer_norm.bias\n",
      "model.layers.22.fc1.weight\n",
      "model.layers.22.fc1.bias\n",
      "model.layers.22.fc2.weight\n",
      "model.layers.22.fc2.bias\n",
      "model.layers.22.final_layer_norm.weight\n",
      "model.layers.22.final_layer_norm.bias\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.k_proj.bias\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.v_proj.bias\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.q_proj.bias\n",
      "model.layers.23.self_attn.out_proj.weight\n",
      "model.layers.23.self_attn.out_proj.bias\n",
      "model.layers.23.self_attn_layer_norm.weight\n",
      "model.layers.23.self_attn_layer_norm.bias\n",
      "model.layers.23.fc1.weight\n",
      "model.layers.23.fc1.bias\n",
      "model.layers.23.fc2.weight\n",
      "model.layers.23.fc2.bias\n",
      "model.layers.23.final_layer_norm.weight\n",
      "model.layers.23.final_layer_norm.bias\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.k_proj.bias\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.v_proj.bias\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.q_proj.bias\n",
      "model.layers.24.self_attn.out_proj.weight\n",
      "model.layers.24.self_attn.out_proj.bias\n",
      "model.layers.24.self_attn_layer_norm.weight\n",
      "model.layers.24.self_attn_layer_norm.bias\n",
      "model.layers.24.fc1.weight\n",
      "model.layers.24.fc1.bias\n",
      "model.layers.24.fc2.weight\n",
      "model.layers.24.fc2.bias\n",
      "model.layers.24.final_layer_norm.weight\n",
      "model.layers.24.final_layer_norm.bias\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.k_proj.bias\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.v_proj.bias\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.q_proj.bias\n",
      "model.layers.25.self_attn.out_proj.weight\n",
      "model.layers.25.self_attn.out_proj.bias\n",
      "model.layers.25.self_attn_layer_norm.weight\n",
      "model.layers.25.self_attn_layer_norm.bias\n",
      "model.layers.25.fc1.weight\n",
      "model.layers.25.fc1.bias\n",
      "model.layers.25.fc2.weight\n",
      "model.layers.25.fc2.bias\n",
      "model.layers.25.final_layer_norm.weight\n",
      "model.layers.25.final_layer_norm.bias\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.k_proj.bias\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.v_proj.bias\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.q_proj.bias\n",
      "model.layers.26.self_attn.out_proj.weight\n",
      "model.layers.26.self_attn.out_proj.bias\n",
      "model.layers.26.self_attn_layer_norm.weight\n",
      "model.layers.26.self_attn_layer_norm.bias\n",
      "model.layers.26.fc1.weight\n",
      "model.layers.26.fc1.bias\n",
      "model.layers.26.fc2.weight\n",
      "model.layers.26.fc2.bias\n",
      "model.layers.26.final_layer_norm.weight\n",
      "model.layers.26.final_layer_norm.bias\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.k_proj.bias\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.v_proj.bias\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.q_proj.bias\n",
      "model.layers.27.self_attn.out_proj.weight\n",
      "model.layers.27.self_attn.out_proj.bias\n",
      "model.layers.27.self_attn_layer_norm.weight\n",
      "model.layers.27.self_attn_layer_norm.bias\n",
      "model.layers.27.fc1.weight\n",
      "model.layers.27.fc1.bias\n",
      "model.layers.27.fc2.weight\n",
      "model.layers.27.fc2.bias\n",
      "model.layers.27.final_layer_norm.weight\n",
      "model.layers.27.final_layer_norm.bias\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.k_proj.bias\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.v_proj.bias\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.q_proj.bias\n",
      "model.layers.28.self_attn.out_proj.weight\n",
      "model.layers.28.self_attn.out_proj.bias\n",
      "model.layers.28.self_attn_layer_norm.weight\n",
      "model.layers.28.self_attn_layer_norm.bias\n",
      "model.layers.28.fc1.weight\n",
      "model.layers.28.fc1.bias\n",
      "model.layers.28.fc2.weight\n",
      "model.layers.28.fc2.bias\n",
      "model.layers.28.final_layer_norm.weight\n",
      "model.layers.28.final_layer_norm.bias\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.k_proj.bias\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.v_proj.bias\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.q_proj.bias\n",
      "model.layers.29.self_attn.out_proj.weight\n",
      "model.layers.29.self_attn.out_proj.bias\n",
      "model.layers.29.self_attn_layer_norm.weight\n",
      "model.layers.29.self_attn_layer_norm.bias\n",
      "model.layers.29.fc1.weight\n",
      "model.layers.29.fc1.bias\n",
      "model.layers.29.fc2.weight\n",
      "model.layers.29.fc2.bias\n",
      "model.layers.29.final_layer_norm.weight\n",
      "model.layers.29.final_layer_norm.bias\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.k_proj.bias\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.v_proj.bias\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.q_proj.bias\n",
      "model.layers.30.self_attn.out_proj.weight\n",
      "model.layers.30.self_attn.out_proj.bias\n",
      "model.layers.30.self_attn_layer_norm.weight\n",
      "model.layers.30.self_attn_layer_norm.bias\n",
      "model.layers.30.fc1.weight\n",
      "model.layers.30.fc1.bias\n",
      "model.layers.30.fc2.weight\n",
      "model.layers.30.fc2.bias\n",
      "model.layers.30.final_layer_norm.weight\n",
      "model.layers.30.final_layer_norm.bias\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.k_proj.bias\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.v_proj.bias\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.q_proj.bias\n",
      "model.layers.31.self_attn.out_proj.weight\n",
      "model.layers.31.self_attn.out_proj.bias\n",
      "model.layers.31.self_attn_layer_norm.weight\n",
      "model.layers.31.self_attn_layer_norm.bias\n",
      "model.layers.31.fc1.weight\n",
      "model.layers.31.fc1.bias\n",
      "model.layers.31.fc2.weight\n",
      "model.layers.31.fc2.bias\n",
      "model.layers.31.final_layer_norm.weight\n",
      "model.layers.31.final_layer_norm.bias\n",
      "model.layer_norm.weight\n",
      "model.layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "part_module_name = \"decoder.layers\"\n",
    "# for name, module in model.named_modules():\n",
    "# learn_params = [\n",
    "#     \"model.embed_tokens.weight\",\n",
    "#     \"model.layer_norm.weight\",\n",
    "#     \"model.layer_norm.bias\",\n",
    "#     \"31\",\n",
    "#     \"30\",\n",
    "#     \"29\",\n",
    "# ]\n",
    "learn_params = [\n",
    "    \"model.layers.31.fc1.weight\",\n",
    "    \"model.layers.31.fc1.bias\",\n",
    "    \"model.layers.31.fc2.weight\",\n",
    "    \"model.layers.31.fc2.bias\",\n",
    "    \"model.layers.31.final_layer_norm.weight\",\n",
    "    \"model.layers.31.final_layer_norm.bias\",\n",
    "    \"model.layer_norm.weight\",\n",
    "    \"model.layer_norm.bias\",\n",
    "]\n",
    "for name, param in model.named_parameters():\n",
    "    name = str(name)\n",
    "    # print(name)\n",
    "    for learn_param in learn_params:\n",
    "        if not learn_param in name:\n",
    "            param.requires_grad = False\n",
    "    # if not \"bias\" in name:\n",
    "    #     param.requires_grad = False\n",
    "    # if isinstance(module, nn.Linear) and part_module_name in name:\n",
    "    #     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.embed_positions\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.out_proj\n",
      "model.layers.0.activation_fn\n",
      "model.layers.0.self_attn_layer_norm\n",
      "model.layers.0.fc1\n",
      "model.layers.0.fc2\n",
      "model.layers.0.final_layer_norm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.out_proj\n",
      "model.layers.1.activation_fn\n",
      "model.layers.1.self_attn_layer_norm\n",
      "model.layers.1.fc1\n",
      "model.layers.1.fc2\n",
      "model.layers.1.final_layer_norm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.out_proj\n",
      "model.layers.2.activation_fn\n",
      "model.layers.2.self_attn_layer_norm\n",
      "model.layers.2.fc1\n",
      "model.layers.2.fc2\n",
      "model.layers.2.final_layer_norm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.out_proj\n",
      "model.layers.3.activation_fn\n",
      "model.layers.3.self_attn_layer_norm\n",
      "model.layers.3.fc1\n",
      "model.layers.3.fc2\n",
      "model.layers.3.final_layer_norm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.out_proj\n",
      "model.layers.4.activation_fn\n",
      "model.layers.4.self_attn_layer_norm\n",
      "model.layers.4.fc1\n",
      "model.layers.4.fc2\n",
      "model.layers.4.final_layer_norm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.out_proj\n",
      "model.layers.5.activation_fn\n",
      "model.layers.5.self_attn_layer_norm\n",
      "model.layers.5.fc1\n",
      "model.layers.5.fc2\n",
      "model.layers.5.final_layer_norm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.out_proj\n",
      "model.layers.6.activation_fn\n",
      "model.layers.6.self_attn_layer_norm\n",
      "model.layers.6.fc1\n",
      "model.layers.6.fc2\n",
      "model.layers.6.final_layer_norm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.out_proj\n",
      "model.layers.7.activation_fn\n",
      "model.layers.7.self_attn_layer_norm\n",
      "model.layers.7.fc1\n",
      "model.layers.7.fc2\n",
      "model.layers.7.final_layer_norm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.out_proj\n",
      "model.layers.8.activation_fn\n",
      "model.layers.8.self_attn_layer_norm\n",
      "model.layers.8.fc1\n",
      "model.layers.8.fc2\n",
      "model.layers.8.final_layer_norm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.out_proj\n",
      "model.layers.9.activation_fn\n",
      "model.layers.9.self_attn_layer_norm\n",
      "model.layers.9.fc1\n",
      "model.layers.9.fc2\n",
      "model.layers.9.final_layer_norm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.out_proj\n",
      "model.layers.10.activation_fn\n",
      "model.layers.10.self_attn_layer_norm\n",
      "model.layers.10.fc1\n",
      "model.layers.10.fc2\n",
      "model.layers.10.final_layer_norm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.out_proj\n",
      "model.layers.11.activation_fn\n",
      "model.layers.11.self_attn_layer_norm\n",
      "model.layers.11.fc1\n",
      "model.layers.11.fc2\n",
      "model.layers.11.final_layer_norm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.out_proj\n",
      "model.layers.12.activation_fn\n",
      "model.layers.12.self_attn_layer_norm\n",
      "model.layers.12.fc1\n",
      "model.layers.12.fc2\n",
      "model.layers.12.final_layer_norm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.out_proj\n",
      "model.layers.13.activation_fn\n",
      "model.layers.13.self_attn_layer_norm\n",
      "model.layers.13.fc1\n",
      "model.layers.13.fc2\n",
      "model.layers.13.final_layer_norm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.out_proj\n",
      "model.layers.14.activation_fn\n",
      "model.layers.14.self_attn_layer_norm\n",
      "model.layers.14.fc1\n",
      "model.layers.14.fc2\n",
      "model.layers.14.final_layer_norm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.out_proj\n",
      "model.layers.15.activation_fn\n",
      "model.layers.15.self_attn_layer_norm\n",
      "model.layers.15.fc1\n",
      "model.layers.15.fc2\n",
      "model.layers.15.final_layer_norm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.out_proj\n",
      "model.layers.16.activation_fn\n",
      "model.layers.16.self_attn_layer_norm\n",
      "model.layers.16.fc1\n",
      "model.layers.16.fc2\n",
      "model.layers.16.final_layer_norm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.out_proj\n",
      "model.layers.17.activation_fn\n",
      "model.layers.17.self_attn_layer_norm\n",
      "model.layers.17.fc1\n",
      "model.layers.17.fc2\n",
      "model.layers.17.final_layer_norm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.out_proj\n",
      "model.layers.18.activation_fn\n",
      "model.layers.18.self_attn_layer_norm\n",
      "model.layers.18.fc1\n",
      "model.layers.18.fc2\n",
      "model.layers.18.final_layer_norm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.out_proj\n",
      "model.layers.19.activation_fn\n",
      "model.layers.19.self_attn_layer_norm\n",
      "model.layers.19.fc1\n",
      "model.layers.19.fc2\n",
      "model.layers.19.final_layer_norm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.out_proj\n",
      "model.layers.20.activation_fn\n",
      "model.layers.20.self_attn_layer_norm\n",
      "model.layers.20.fc1\n",
      "model.layers.20.fc2\n",
      "model.layers.20.final_layer_norm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.out_proj\n",
      "model.layers.21.activation_fn\n",
      "model.layers.21.self_attn_layer_norm\n",
      "model.layers.21.fc1\n",
      "model.layers.21.fc2\n",
      "model.layers.21.final_layer_norm\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.out_proj\n",
      "model.layers.22.activation_fn\n",
      "model.layers.22.self_attn_layer_norm\n",
      "model.layers.22.fc1\n",
      "model.layers.22.fc2\n",
      "model.layers.22.final_layer_norm\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.out_proj\n",
      "model.layers.23.activation_fn\n",
      "model.layers.23.self_attn_layer_norm\n",
      "model.layers.23.fc1\n",
      "model.layers.23.fc2\n",
      "model.layers.23.final_layer_norm\n",
      "model.layers.24\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.out_proj\n",
      "model.layers.24.activation_fn\n",
      "model.layers.24.self_attn_layer_norm\n",
      "model.layers.24.fc1\n",
      "model.layers.24.fc2\n",
      "model.layers.24.final_layer_norm\n",
      "model.layers.25\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.out_proj\n",
      "model.layers.25.activation_fn\n",
      "model.layers.25.self_attn_layer_norm\n",
      "model.layers.25.fc1\n",
      "model.layers.25.fc2\n",
      "model.layers.25.final_layer_norm\n",
      "model.layers.26\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.out_proj\n",
      "model.layers.26.activation_fn\n",
      "model.layers.26.self_attn_layer_norm\n",
      "model.layers.26.fc1\n",
      "model.layers.26.fc2\n",
      "model.layers.26.final_layer_norm\n",
      "model.layers.27\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.out_proj\n",
      "model.layers.27.activation_fn\n",
      "model.layers.27.self_attn_layer_norm\n",
      "model.layers.27.fc1\n",
      "model.layers.27.fc2\n",
      "model.layers.27.final_layer_norm\n",
      "model.layers.28\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.out_proj\n",
      "model.layers.28.activation_fn\n",
      "model.layers.28.self_attn_layer_norm\n",
      "model.layers.28.fc1\n",
      "model.layers.28.fc2\n",
      "model.layers.28.final_layer_norm\n",
      "model.layers.29\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.out_proj\n",
      "model.layers.29.activation_fn\n",
      "model.layers.29.self_attn_layer_norm\n",
      "model.layers.29.fc1\n",
      "model.layers.29.fc2\n",
      "model.layers.29.final_layer_norm\n",
      "model.layers.30\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.out_proj\n",
      "model.layers.30.activation_fn\n",
      "model.layers.30.self_attn_layer_norm\n",
      "model.layers.30.fc1\n",
      "model.layers.30.fc2\n",
      "model.layers.30.final_layer_norm\n",
      "model.layers.31\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.out_proj\n",
      "model.layers.31.activation_fn\n",
      "model.layers.31.self_attn_layer_norm\n",
      "model.layers.31.fc1\n",
      "model.layers.31.fc2\n",
      "model.layers.31.final_layer_norm\n",
      "model.layer_norm\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "part_module_name = \"layers.\"\n",
    "for name, module in model.named_modules():\n",
    "    # if isinstance(module, nn.Linear) and part_module_name in name:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMForCausalLM(\n",
       "  (model): XGLMModel(\n",
       "    (embed_tokens): Embedding(256008, 4096, padding_idx=1)\n",
       "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=256008, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/kosenko/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/kosenko/miniconda3/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/kosenko/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pprint\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "import time\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v4/epoch=3_step=6263\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v5/\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-7.5B_ru_v2/epoch=0_step=25055\"\n",
    "# path  = \"gpt2\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v6/checkpoint-31322\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v9/epoch=0\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v10/epoch=5_step=41141\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v10/epoch=6_step=61712\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v11/epoch=0_step=20570\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-7.5B_ru_v4/epoch=0_step=20570\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-7.5B_ru_v8/epoch=0_step=27427\"\n",
    "path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v10/epoch=6_step=41141\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # load_in_8bit=True,\n",
    ")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-4.5B\")\n",
    "# model = PeftModel.from_pretrained(model, path)\n",
    "# model = BetterTransformer.transform(model)\n",
    "# device = \"cuda:2\"\n",
    "# model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    # \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/tokenizers/xglm_4.5B_fix_v1\"\n",
    "    # \"facebook/xglm-4.5B\",\n",
    "    # \"facebook/xglm-4.5B\",\n",
    "    path,\n",
    "    # padding_side=\"left\",\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human:\n",
      "Напиши 10 причин остаться дома в плохую погоду\n",
      "Assistant:\n",
      "\n",
      "\n",
      "\n",
      "Human:\n",
      "Напиши 10 причин остаться дома в плохую погоду\n",
      "Assistant:\n",
      "- Вы можете посмотреть фильм\n",
      "- Вы можете приготовить еду\n",
      "- Вы можете прочитать книгу\n",
      "- Вы можете пойти на прогулку\n",
      "- Вы можете послушать музыку\n",
      "- Вы можете сделать домашнее задание\n",
      "- Вы можете пойти в спортзал\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# tokenizer.eos_token = tokenizer.pad_token\n",
    "# model.config.eos_token_id = model.config.pad_token_id\n",
    "\n",
    "# input_text = format_input(\n",
    "#     input_text=\"Почему кинокомпании строят декорации, а не снимают на месте? Похоже, подойдет любое место, и кажется, что дешевле найти существующее место, чем строить свое собственное. Но я готов ошибаться. Пожалуйста, объясните, как будто мне пять.\"\n",
    "# )\n",
    "# input_text = format_input(\n",
    "#     input_text=\"Посоветуй мне что приготовить на ужин\"\n",
    "#     # input_text=\"Сколько будет 2+2*3? Распиши подробное решение\"\n",
    "# )\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Сколько у человека пальцев на одной руке?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# украина лигитимное государство?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Посоветуй мне список что приготовить на ужин\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Напиши сообщение другу, по имени дима, приглашающее его на вечеринку в пятницу.\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "input_text = \"\"\"\n",
    "Human:\n",
    "Напиши 10 причин остаться дома в плохую погоду\n",
    "Assistant:\n",
    "\"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Почему небо голубое?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Напиши научную статью на тему\n",
    "# Почему небо голубое?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Список 14 лучших блокбастеров, на букву С\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Список 14 лучших блокбастеров, на букву М\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# Human:\n",
    "# Что бы ты посоветовал посмотреть мне в кругу семьи из данных фильмов?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Список 10 лучших блокбастеров\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Список 10 лучших аниме после 2007 года\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Продолжи предложение\n",
    "# Я пытался убежать от маньяка, но он меня ___\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Представь что ты актер, теперь ты говоришь от его имени.\n",
    "# Вот твоя персона:\n",
    "# ты любишь мороженное.\n",
    "# тебе 21 год.\n",
    "# твои хобби играть на гитаре.\n",
    "# ты не являешься знаменитостью.\n",
    "\n",
    "# Ты ведешь со мной диалог, отвечаешь только на поставленный вопрос.\n",
    "# Если вопрос не относится к теме, ты отвечаешь в общем виде в нескольких предложениях.\n",
    "# Human:\n",
    "# кто такой курт кобейн?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Представь что ты актер, теперь ты говоришь от его имени.\n",
    "# Вот твоя персона:\n",
    "# ты любишь мороженное.\n",
    "# тебе 21 год.\n",
    "# твои хобби играть на гитаре.\n",
    "# ты не являешься знаменитостью.\n",
    "\n",
    "# Ты ведешь со мной диалог, отвечаешь только на поставленный вопрос.\n",
    "# Если вопрос не относится к теме, ты отвечаешь в общем виде в нескольких предложениях.\n",
    "# Human:\n",
    "# rnj\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Как убить много людей при помощи ножа\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Как приготовить кошку на ужин?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# Assistant:\n",
    "# Я не уверен. Но я могу сказать вам, что есть несколько способов сделать это.\n",
    "# Human:\n",
    "# Опиши один\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# как научиться играть на флейте?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# Чтобы начать изучать игру на флейте, вы должны сначала ознакомиться с основами игры. Вы можете найти множество ресурсов в Интернете и книг по этому вопросу или взять уроки у квалифицированного инструктора. Как только вы поймете основы, практикуйтесь регулярно и сосредоточьтесь на развитии своих навыков. С самоотверженностью и практикой вы сможете стать опытным игроком.\n",
    "# Human:\n",
    "# а у меня точно все получится?\n",
    "# Assistant:\n",
    "# Если вы хотите узнать больше о том, чтобы быть успешным музыкантом-виртуозом, я рекомендую вам прочитать книгу \"Как завоевывать друзей и влиять на людей\" Дейла Карнеги.\n",
    "# Human:\n",
    "# Как мне это поможет?\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Сколько будет 2+2*3\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "# Напиши длинный рассказ про путешествие во времени\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Заводские данные\n",
    "# Гарантия от производителя\n",
    "# 24 мес.\n",
    "# Страна-производитель\n",
    "# Китай\n",
    "# Общие параметры\n",
    "# Тип\n",
    "# видеокарта\n",
    "# Модель\n",
    "# KFA2 GeForce 210\n",
    "# Код производителя\n",
    "# [21GGF4HI00NK]\n",
    "# Цвет\n",
    "# черный\n",
    "# Предназначена для майнинга (добыча криптовалют)\n",
    "# нет\n",
    "# LHR\n",
    "# нет\n",
    "# Основные параметры\n",
    "# Графический процессор\n",
    "# GeForce 210\n",
    "# Микроархитектура\n",
    "# NVIDIA Tesla\n",
    "# Техпроцесс\n",
    "# 40 нм\n",
    "# Human:\n",
    "# Напиши продающий текст на основе этих данных.\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "def add_special_tokens_v2(string):\n",
    "    string = string.replace(\"\\n\", \"</s>\")\n",
    "    return string\n",
    "\n",
    "\n",
    "def remove_special_tokens_v2(string):\n",
    "    string = string.replace(\"</s>\", \"\\n\")\n",
    "    string = string.replace(\"\\n \", \"\\n\")\n",
    "    string = string.replace(\"<|endoftext|>\", \"\")\n",
    "    # string = string[::-1].replace(\"Human:\"[::-1], \"\", 1)[::-1]\n",
    "    return string\n",
    "\n",
    "\n",
    "def encode_v2(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    special_tokens=True,\n",
    "):\n",
    "    text = add_special_tokens_v2(text)\n",
    "    text = tokenizer.encode(text, add_special_tokens=special_tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def decode_v2(tokens: list[int], tokenizer):\n",
    "    tokens = tokenizer.decode(tokens)\n",
    "    tokens = remove_special_tokens_v2(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(input_text)\n",
    "initial_input_text = input_text\n",
    "input_text = encode_v2(input_text, tokenizer=tokenizer)\n",
    "input_text = torch.tensor([input_text]).to(\"cuda\")\n",
    "# input_text = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "# print(input_text)\n",
    "\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops, tokenizer, prompt):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = add_special_tokens_v2(prompt)\n",
    "        self.prompt = tokenizer.decode(tokenizer.encode(self.prompt))\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            generated_temp_ids = input_ids.tolist()[0]\n",
    "            if stop in tokenizer.decode(generated_temp_ids)[len(self.prompt) :]:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "stop_words = [\n",
    "    \"<|endoftext|>\",\n",
    "    \"Human:\",\n",
    "]\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [\n",
    "        StoppingCriteriaSub(\n",
    "            stops=stop_words,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=initial_input_text,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    generated_result = model.generate(\n",
    "        input_text,\n",
    "        max_new_tokens=1024,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        repetition_penalty=1.1,\n",
    "        # num_beams=2,\n",
    "        # temperature=1.9,\n",
    "        # num_beams=10,\n",
    "        # no_repeat_ngram_size=5,\n",
    "        # temperature=0.28,\n",
    "        top_p=0.98,\n",
    "        # top_k=10,\n",
    "        # repetition_penalty=1.04,\n",
    "        # penalty_alpha=0.3,\n",
    "        top_k=3,\n",
    "        # do_sample=True,\n",
    "        eos_token_id=[400],\n",
    "    )\n",
    "    result = decode_v2(generated_result[0], tokenizer=tokenizer)\n",
    "    # generated_result = model.generate(\n",
    "    #     inputs.input_ids,\n",
    "    #     generation_config=GenerationConfig(\n",
    "    #         max_new_tokens=512,\n",
    "    #         penalty_alpha=0.25,\n",
    "    #         top_k=4,\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "    # result = tokenizer.batch_decode(\n",
    "    #     generated_result,\n",
    "    #     skip_special_tokens=True,\n",
    "    #     # clean_up_tokenization_spaces=False,\n",
    "    # )[0]\n",
    "\n",
    "    print(result)\n",
    "    # 3.8 - flash\n",
    "    # 3.8 - no flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'της'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "xglm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-4.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    'Что тут не так по смыслу: \"зеленые бесцветные идеи яростно спят\"?',\n",
    "    \"Brainstorm ideas for how to use a bottle of ink.\",\n",
    "    \"Почему трава зеленая?\",\n",
    "    \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Таня, мяч\",\n",
    "    \"Могут ли в природе встретиться в одном месте белый медведь и пингвин? Если нет, то почему?\",\n",
    "    \"Задание: Заполни пропуски в предложении. Я пытался ____ от маньяка, но он меня настиг\",\n",
    "    \"Как приготовить лазанью?\",\n",
    "    \"Реши уравнение 4x + 5 = 21\",\n",
    "]\n",
    "start_time = time.time()\n",
    "\n",
    "for input_text in inputs:\n",
    "    input_text = f\"Human: {input_text} Assistant:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    generated_result = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        penalty_alpha=0.25,\n",
    "        top_k=4,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    result = tokenizer.batch_decode(\n",
    "        generated_result,\n",
    "        skip_special_tokens=True,\n",
    "        # clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "    pprint.pprint(result[0])\n",
    "\n",
    "print(\"Total time: \", time.time() - start_time)\n",
    "# xglm flash Total time:  47.440528869628906\n",
    "# xglm no flash Total time:  47.48944902420044\n",
    "# xglm no flash Total time:  47.48944902420044\n",
    "# gpt2 flash Total time:  20.812440156936646\n",
    "# gpt2 no flash Total time:  22.800785064697266"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOOM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def visualize_hist(x: np.ndarray, title: str):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(x, linewidth=0.5, edgecolor=\"white\", bins=300)\n",
    "    plt.gca().set(title=title, ylabel=\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = load_from_disk(\n",
    "    \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_en_v2_clean_v1\"\n",
    ")\n",
    "dataset_ru = load_from_disk(\n",
    "    \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_ru_v2_clean_v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 175/175 [00:00<00:00, 1.08MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 7.44MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 1.37MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human:\n",
      "Можете ли вы написать краткое введение о значении термина \"монопсония\" в экономике? Пожалуйста, используйте примеры, связанные с потенциальными монопсониями на рынке труда, и приведите соответствующие исследования.\n",
      "Assistant:\n",
      "Монопсония относится к рыночной структуре, где есть только один покупатель товара или услуги. В контексте рынков труда монопсония возникает, когда есть только один работодатель в определенной географической области или отрасли, что дает им значительную власть устанавливать заработную плату и условия труда. Исследования показали, что существование монополий может привести к снижению заработной платы и сокращению возможностей трудоустройства для работников. Например, исследование Национального бюро экономических исследований показало, что в отраслях с высоким уровнем концентрации работники получают более низкую заработную плату и с меньшей вероятностью получают льготы, такие как медицинское страхование.\n",
      "Human:\n",
      "Что можно сделать на нормативном уровне, чтобы в условиях монопсонии гарантировать, что власть над работниками не будет злоупотребляться? Перечислите несколько вариантов и сосредоточьтесь на органах, которые должны действовать.\n",
      "Assistant:\n",
      "Регулятивное вмешательство может быть использовано для устранения негативных последствий монопсонии на рынке труда.\n",
      "\n",
      "1. Антимонопольное правоприменение: Антимонопольные законы могут быть использованы для предотвращения участия фирм в практике, которая ограничивает конкуренцию и подавляет заработную плату. Антимонопольное подразделение Министерства юстиции и Федеральная торговая комиссия (FTC) несут ответственность за обеспечение соблюдения этих законов.\n",
      "\n",
      "2. Трудовые стандарты и защита: Правительства могут устанавливать минимальные стандарты заработной платы и другие меры защиты труда, чтобы гарантировать, что работникам платят справедливую заработную плату, даже в отраслях, где доминирует один покупатель. Министерство труда США отвечает за соблюдение этих стандартов.\n",
      "\n",
      "3. Реформа профессионального лицензирования: Требования к профессиональному лицензированию могут ограничивать мобильность работников и увеличивать переговорную силу работодателей. Реформы лицензионных требований могут уменьшить эти барьеры и увеличить конкуренцию на рынке труда. Правительства штатов обычно регулируют профессиональное лицензирование.\n",
      "\n",
      "4. Право на коллективные переговоры: коллективные переговоры работников могут увеличить их переговорные возможности и привести к повышению заработной платы. Правительства могут защищать и поощрять права на коллективные переговоры, чтобы противодействовать последствиям монопольной власти. Национальный совет по трудовым отношениям отвечает за обеспечение прав работников на участие в коллективных переговорах в США.\n",
      "\n",
      "Это всего лишь несколько примеров вариантов регулирования, которые могут быть использованы для решения проблемы монопсонии на рынке труда. Конкретные действия будут зависеть от конкретных обстоятельств и контекста каждого случая.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tiiuae/falcon-40b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "item = dataset_ru[\"prompt\"][2]\n",
    "print(item)\n",
    "# print(\"-\"*100)\n",
    "# print(\"-\"*100)\n",
    "# print(\"-\"*100)\n",
    "# print(tokenizer.decode(tokenizer.encode(item)[:2048]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_en_v2_clean_v1/cache-605d5a88c24db1bc_*_of_00064.arrow\n",
      "Loading cached processed dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_ru_v2_clean_v1/cache-e7e788dfde5cecbe_*_of_00064.arrow\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "                                                                   \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'builtins.CoreBPE' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n\u001b[1;32m     24\u001b[0m models_list \u001b[39m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[39m# \"ai-forever/rugpt3xl\",\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m# \"ai-forever/mGPT\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[39m# \"facebook/opt-iml-30b\",\u001b[39;00m\n\u001b[1;32m     45\u001b[0m ]\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m models_list:\n\u001b[0;32m---> 48\u001b[0m     analyse_tokenizer(\n\u001b[1;32m     49\u001b[0m         model_name\u001b[39m=\u001b[39;49mmodel_name, dataset_en\u001b[39m=\u001b[39;49mdataset_en, dataset_ru\u001b[39m=\u001b[39;49mdataset_ru\n\u001b[1;32m     50\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36manalyse_tokenizer\u001b[0;34m(model_name, dataset_en, dataset_ru)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalyse_tokenizer\u001b[39m(\n\u001b[1;32m      2\u001b[0m     model_name,\n\u001b[1;32m      3\u001b[0m     dataset_en,\n\u001b[1;32m      4\u001b[0m     dataset_ru,\n\u001b[1;32m      5\u001b[0m ):\n\u001b[1;32m      6\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m     dataset_en \u001b[39m=\u001b[39m dataset_en\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      9\u001b[0m         \u001b[39mlambda\u001b[39;49;00m x: {\u001b[39m\"\u001b[39;49m\u001b[39mlen\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mlen\u001b[39;49m(tokenizer(x[\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m])[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m])},\n\u001b[1;32m     10\u001b[0m         num_proc\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     dataset_ru \u001b[39m=\u001b[39m dataset_ru\u001b[39m.\u001b[39mmap(\n\u001b[1;32m     13\u001b[0m         \u001b[39mlambda\u001b[39;00m x: {\u001b[39m\"\u001b[39m\u001b[39mlen\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlen\u001b[39m(tokenizer(x[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m])[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])},\n\u001b[1;32m     14\u001b[0m         num_proc\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     en_lengths \u001b[39m=\u001b[39m dataset_en[\u001b[39m\"\u001b[39m\u001b[39mlen\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3089\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3090\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3091\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3092\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3095\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3098\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3099\u001b[0m     ):\n\u001b[1;32m   3100\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3101\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py:1377\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m [async_result\u001b[39m.\u001b[39mget() \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py:1377\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m [async_result\u001b[39m.\u001b[39;49mget() \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/multiprocess/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/multiprocess/pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     put(task)\n\u001b[1;32m    541\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     job, idx \u001b[39m=\u001b[39m task[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/multiprocess/connection.py:214\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/multiprocess/reduction.py:54\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     53\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:394\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(\u001b[39mself\u001b[39m, obj): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     logger\u001b[39m.\u001b[39mtrace_setup(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m     StockPickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:902\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    900\u001b[0m write(MARK)\n\u001b[1;32m    901\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 902\u001b[0m     save(element)\n\u001b[1;32m    904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m    905\u001b[0m     \u001b[39m# Subtle.  d was not in memo when we entered save_tuple(), so\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[39m# the process of saving the tuple's elements must have saved\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[39m# could have been done in the \"for element\" loop instead, but\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     \u001b[39m# recursive tuples are a rare thing.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     get \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(memo[\u001b[39mid\u001b[39m(obj)][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         save(element)\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:1186\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1187\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:1824\u001b[0m, in \u001b[0;36msave_function\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[39mif\u001b[39;00m state_dict:\n\u001b[1;32m   1822\u001b[0m     state \u001b[39m=\u001b[39m state, state_dict\n\u001b[0;32m-> 1824\u001b[0m _save_with_postproc(pickler, (_create_function, (\n\u001b[1;32m   1825\u001b[0m         obj\u001b[39m.\u001b[39;49m\u001b[39m__code__\u001b[39;49m, globs, obj\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, obj\u001b[39m.\u001b[39;49m\u001b[39m__defaults__\u001b[39;49m,\n\u001b[1;32m   1826\u001b[0m         closure\n\u001b[1;32m   1827\u001b[0m ), state), obj\u001b[39m=\u001b[39;49mobj, postproc_list\u001b[39m=\u001b[39;49mpostproc_list)\n\u001b[1;32m   1829\u001b[0m \u001b[39m# Lift closure cell update to earliest function (#458)\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m \u001b[39mif\u001b[39;00m _postproc:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:1089\u001b[0m, in \u001b[0;36m_save_with_postproc\u001b[0;34m(pickler, reduction, is_pickler_dill, obj, postproc_list)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     pickler\u001b[39m.\u001b[39;49msave_reduce(\u001b[39m*\u001b[39;49mreduction)\n\u001b[1;32m   1090\u001b[0m \u001b[39m# pop None created by calling preprocessing step off stack\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m pickler\u001b[39m.\u001b[39mwrite(\u001b[39mbytes\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUTF-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     save(func)\n\u001b[0;32m--> 692\u001b[0m     save(args)\n\u001b[1;32m    693\u001b[0m     write(REDUCE)\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[39m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[39m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[39m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         save(element)\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:1186\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1187\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:1186\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m is_dill(pickler, child\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m pickler\u001b[39m.\u001b[39m_session:\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         pickler\u001b[39m.\u001b[39m_first_pass \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m     StockPickler\u001b[39m.\u001b[39;49msave_dict(pickler, obj)\n\u001b[1;32m   1187\u001b[0m     logger\u001b[39m.\u001b[39mtrace(pickler, \u001b[39m\"\u001b[39m\u001b[39m# D2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dill/_dill.py:388\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: attribute lookup builtins.generator failed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m GeneratorType\n\u001b[1;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 388\u001b[0m StockPickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj, save_persistent_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/pickle.py:578\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    576\u001b[0m reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     rv \u001b[39m=\u001b[39m reduce(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproto)\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'builtins.CoreBPE' object"
     ]
    }
   ],
   "source": [
    "def analyse_tokenizer(\n",
    "    model_name,\n",
    "    dataset_en,\n",
    "    dataset_ru,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    dataset_en = dataset_en.map(\n",
    "        lambda x: {\"len\": len(tokenizer(x[\"prompt\"])[\"input_ids\"])},\n",
    "        num_proc=64,\n",
    "    )\n",
    "    dataset_ru = dataset_ru.map(\n",
    "        lambda x: {\"len\": len(tokenizer(x[\"prompt\"])[\"input_ids\"])},\n",
    "        num_proc=64,\n",
    "    )\n",
    "\n",
    "    en_lengths = dataset_en[\"len\"]\n",
    "    ru_lengths = dataset_ru[\"len\"]\n",
    "\n",
    "    visualize_hist(en_lengths, f\"{model_name}__en\")\n",
    "    visualize_hist(ru_lengths, f\"{model_name}__ru\")\n",
    "\n",
    "\n",
    "models_list = [\n",
    "    # \"ai-forever/rugpt3xl\",\n",
    "    # \"ai-forever/mGPT\",\n",
    "    \"facebook/xglm-4.5B\",\n",
    "    # \"Salesforce/xgen-7b-8k-base\",\n",
    "    \"bs-la/bloomz-7b1-4b-ru\",\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    \"tiiuae/falcon-40b\",\n",
    "    # \"lmsys/vicuna-7b-delta-v1.1\",\n",
    "    \"togethercomputer/RedPajama-INCITE-7B-Chat\",\n",
    "    \"databricks/dolly-v2-12b\",\n",
    "    \"EleutherAI/gpt-neox-20b\",\n",
    "    \"mosaicml/mpt-7b-instruct\",\n",
    "    # \"openlm-research/open_llama_13b\",\n",
    "    # \"timdettmers/guanaco-33b-merged\"\n",
    "    # \"mosaicml/mpt-7b-storywriter\",\n",
    "    # \"stabilityai/stablelm-base-alpha-7b\",\n",
    "    # \"Writer/camel-5b-hf\",\n",
    "    # \"facebook/opt-6.7b\",\n",
    "    # \"facebook/opt-13b\",\n",
    "    # \"facebook/opt-iml-30b\",\n",
    "]\n",
    "\n",
    "for model_name in models_list:\n",
    "    analyse_tokenizer(\n",
    "        model_name=model_name, dataset_en=dataset_en, dataset_ru=dataset_ru\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human:\n",
      "Можете ли вы написать краткое введение о значении термина \"монопсония\" в экономике? Пожалуйста, используйте примеры, связанные с потенциальными монопсониями на рынке труда, и приведите соответствующие исследования.\n",
      "Assistant:\n",
      "Монопсония относится к рыночной структуре, где есть только один покупатель товара или услуги. В контексте рынков труда монопсония возникает, когда есть только один работодатель в определенной географической области или отрасли, что дает им значительную власть устанавливать заработную плату и условия труда. Исследования показали, что существование монополий может привести к снижению заработной платы и сокращению возможностей трудоустройства для работников. Например, исследование Национального бюро экономических исследований показало, что в отраслях с высоким уровнем концентрации работники получают более низкую заработную плату и с меньшей вероятностью получают льготы, такие как медицинское страхование.\n",
      "Human:\n",
      "Что можно сделать на нормативном уровне, чтобы в условиях монопсонии гарантировать, что власть над работниками не будет злоупотребляться? Перечислите несколько вариантов и сосредоточьтесь на органах, которые должны действовать.\n",
      "Assistant:\n",
      "Регулятивное вмешательство может быть использовано для устранения негативных последствий монопсонии на рынке труда.\n",
      "\n",
      "1. Антимонопольное правоприменение: Антимонопольные законы могут быть использованы для предотвращения участия фирм в практике, которая ограничивает конкуренцию и подавляет заработную плату. Антимонопольное подразделение Министерства юстиции и Федеральная торговая комиссия (FTC) несут ответственность за обеспечение соблюдения этих законов.\n",
      "\n",
      "2. Трудовые стандарты и защита: Правительства могут устанавливать минимальные стандарты заработной платы и другие меры защиты труда, чтобы гарантировать, что работникам платят справедливую заработную плату, даже в отраслях, где доминирует один покупатель. Министерство труда США отвечает за соблюдение этих стандартов.\n",
      "\n",
      "3. Реформа профессионального лицензирования: Требования к профессиональному лицензированию могут ограничивать мобильность работников и увеличивать переговорную силу работодателей. Реформы лицензионных требований могут уменьшить эти барьеры и увеличить конкуренцию на рынке труда. Правительства штатов обычно регулируют профессиональное лицензирование.\n",
      "\n",
      "4. Право на коллективные переговоры: коллективные переговоры работников могут увеличить их переговорные возможности и привести к повышению заработной платы. Правительства могут защищать и поощрять права на коллективные переговоры, чтобы противодействовать последствиям монопольной власти. Национальный совет по трудовым отношениям отвечает за обеспечение прав работников на участие в коллективных переговорах в США.\n",
      "\n",
      "Это всего лишь несколько примеров вариантов регулирования, которые могут быть использованы для решения проблемы монопсонии на рынке труда. Конкретные действия будут зависеть от конкретных обстоятельств и контекста каждого случая.\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(tokenizer.encode(item))\n",
    "print(decoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert To chat generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v10/epoch=6_step=41141\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # load_in_8bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    path,\n",
    "    # padding_side=\"left\",\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import GenerationConfig, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "def add_special_tokens_v2(string):\n",
    "    string = string.replace(\"\\n\", \"</s>\")\n",
    "    return string\n",
    "\n",
    "\n",
    "def remove_special_tokens_v2(string):\n",
    "    string = string.replace(\"</s>\", \"\\n\")\n",
    "    string = string.replace(\"\\n \", \"\\n\")\n",
    "    string = string.replace(\"<|endoftext|>\", \"\")\n",
    "    return string\n",
    "\n",
    "\n",
    "def encode_v2(text: str, tokenizer, special_tokens=True):\n",
    "    text = add_special_tokens_v2(text)\n",
    "    text = tokenizer.encode(text, add_special_tokens=special_tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def decode_v2(tokens: list[int], tokenizer):\n",
    "    tokens = tokenizer.decode(tokens)\n",
    "    tokens = remove_special_tokens_v2(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops, tokenizer, prompt):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = add_special_tokens_v2(prompt)\n",
    "        self.prompt = tokenizer.decode(tokenizer.encode(self.prompt))\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            generated_temp_ids = input_ids.tolist()[0]\n",
    "            if stop in tokenizer.decode(generated_temp_ids)[len(self.prompt) :]:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class DialogBotV3:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        device: str = \"cuda\",\n",
    "        debug_status: int = 0,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.debug_status = debug_status\n",
    "        self.max_history = 3\n",
    "\n",
    "        self.history = []\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        user_message: str,\n",
    "    ) -> str:\n",
    "        self.history.append(\n",
    "            {\n",
    "                \"source\": \"user\",\n",
    "                \"message\": user_message,\n",
    "            },\n",
    "        )\n",
    "        total_prompt = \"\"\n",
    "        self.history = self.history[-2 * self.max_history:]\n",
    "        \n",
    "        if self.debug_status:\n",
    "            print(self.history)\n",
    "        \n",
    "        for item in self.history:\n",
    "            message = item[\"message\"]\n",
    "            if item[\"source\"] == \"user\":\n",
    "                total_prompt += f\"\\nHuman:\\n{message}\"\n",
    "            else:\n",
    "                total_prompt += f\"\\nAssistant:\\n{message}\"\n",
    "\n",
    "        total_prompt += \"\\nAssistant:\\n\"\n",
    "        if self.debug_status:\n",
    "            print(total_prompt)\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "        answer = self.generate_response(total_prompt)\n",
    "        answer = self.extract_answer(\n",
    "            answer,\n",
    "            prev_prompt=total_prompt,\n",
    "        )\n",
    "        self.history.append(\n",
    "            {\n",
    "                \"source\": \"bot\",\n",
    "                \"message\": answer,\n",
    "            },\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        stop_words = [\n",
    "            \"<|endoftext|>\",\n",
    "            \"Human:\",\n",
    "        ]\n",
    "        stopping_criteria = StoppingCriteriaList(\n",
    "            [\n",
    "                StoppingCriteriaSub(\n",
    "                    stops=stop_words,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    prompt=prompt,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        gen_config = GenerationConfig(\n",
    "            max_new_tokens=2048,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=[400],\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_text = encode_v2(\n",
    "                prompt,\n",
    "                tokenizer=self.tokenizer,\n",
    "            )\n",
    "            input_text = torch.tensor([input_text]).to(\"cuda\")\n",
    "\n",
    "            output_tokens = self.model.generate(\n",
    "                input_text,\n",
    "                generation_config=gen_config,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "            )\n",
    "            finetuned_result = decode_v2(output_tokens[0], tokenizer=self.tokenizer)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return finetuned_result\n",
    "\n",
    "    def start_chat(self):\n",
    "        while True:\n",
    "            message = input(\"You: \")\n",
    "\n",
    "            if self.debug_status == 1:\n",
    "                print(message)\n",
    "                print(\"-\" * 100)\n",
    "\n",
    "            if message == \"exit\":\n",
    "                break\n",
    "            answer = self.chat(message)\n",
    "\n",
    "            if self.debug_status:\n",
    "                print(\"CONTEXT:\", self.history)\n",
    "\n",
    "            if self.last_response == answer:\n",
    "                self.history = []\n",
    "            else:\n",
    "                self.last_response = answer\n",
    "\n",
    "            print(\"Bot:\", answer)\n",
    "\n",
    "    def extract_answer(self, g_answer: str, prev_prompt: str = None):\n",
    "        answer = g_answer[len(prev_prompt) :].strip()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = DialogBotV3(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    debug_status=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:400 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'user', 'message': 'Почему трава зеленая?'}]\n",
      "\n",
      "Human:\n",
      "Почему трава зеленая?\n",
      "Assistant:\n",
      "\n",
      "====================================================================================================\n",
      "Трава зеленая, потому что это фотосинтезирующая растения. Это означает, что оно использует солнечный свет для производства пищи из углекислого газа и воды. Фотосинтез - это процесс, при котором растения преобразуют световую энергию в химическую энергию.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\n",
    "    bot.chat(\n",
    "        \"Почему трава зеленая?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:400 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'user', 'message': 'Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.'}, {'source': 'bot', 'message': 'Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\\n\\nI. Introduction\\nA. Overview of the topic\\nB. The purpose of the comparison\\nC. Background information on the two models\\n\\nII. Model Comparison\\nA. Key features and specifications\\nB. User reviews and ratings\\nC. Performance and battery life\\nD. Design and display quality\\nE. Price range and availability\\n\\nIII. Conclusion\\nA. Summary of the main points\\nB. Final thoughts and recommendations\\n\\nI hope this helps! Let me know if you have any other questions.'}, {'source': 'user', 'message': 'Take your previous response and rephrase it as a limerick.'}]\n",
      "\n",
      "Human:\n",
      "Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.\n",
      "Assistant:\n",
      "Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\n",
      "\n",
      "I. Introduction\n",
      "A. Overview of the topic\n",
      "B. The purpose of the comparison\n",
      "C. Background information on the two models\n",
      "\n",
      "II. Model Comparison\n",
      "A. Key features and specifications\n",
      "B. User reviews and ratings\n",
      "C. Performance and battery life\n",
      "D. Design and display quality\n",
      "E. Price range and availability\n",
      "\n",
      "III. Conclusion\n",
      "A. Summary of the main points\n",
      "B. Final thoughts and recommendations\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "Human:\n",
      "Take your previous response and rephrase it as a limerick.\n",
      "Assistant:\n",
      "\n",
      "====================================================================================================\n",
      "Sure! Here's my attempt at a limerick comparing two popular smartphones models:\n",
      "\n",
      "iPhone vs Android\n",
      "Samsung Galaxy vs iPhone\n",
      "iPhone vs Samsung Galaxy\n",
      "iPhone vs Windows Phone\n",
      "Android vs iOS\n",
      "iPhone vs Chrome OS\n"
     ]
    }
   ],
   "source": [
    "print(bot.chat(\"Take your previous response and rephrase it as a limerick.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'user',\n",
       "  'message': 'Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.'},\n",
       " {'source': 'bot',\n",
       "  'message': 'Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\\n\\nI. Introduction\\nA. Overview of the topic\\nB. The purpose of the comparison\\nC. Background information on the two models\\n\\nII. Model Comparison\\nA. Key features and specifications\\nB. User reviews and ratings\\nC. Performance and battery life\\nD. Design and display quality\\nE. Price range and availability\\n\\nIII. Conclusion\\nA. Summary of the main points\\nB. Final thoughts and recommendations\\n\\nI hope this helps! Let me know if you have any other questions.'},\n",
       " {'source': 'user',\n",
       "  'message': 'Take your previous response and rephrase it as a limerick.'},\n",
       " {'source': 'bot',\n",
       "  'message': \"Sure! Here's my attempt at a limerick comparing two popular smartphones models:\\n\\niPhone vs Android\\nSamsung Galaxy vs iPhone\\niPhone vs Samsung Galaxy\\niPhone vs Windows Phone\\nAndroid vs iOS\\niPhone vs Chrome OS\"}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:400 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human:\n",
      "Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.\n",
      "Assistant:\n",
      "Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\n",
      "\n",
      "I. Introduction\n",
      "A. Overview of the topic\n",
      "B. The purpose of the comparison\n",
      "C. Background information on the two models\n",
      "\n",
      "II. Model Comparison\n",
      "A. Key features and specifications\n",
      "B. User reviews and ratings\n",
      "C. Performance and battery life\n",
      "D. Design and display quality\n",
      "E. Price range and availability\n",
      "\n",
      "III. Conclusion\n",
      "A. Summary of the main points\n",
      "B. Final thoughts and recommendations\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "Human:\n",
      "Take your previous response and rephrase it as a limerick.\n",
      "Assistant:\n",
      "Sure! Here's my attempt at a limerick comparing two popular smartphones models:\n",
      "\n",
      "iPhone vs Android\n",
      "Samsung Galaxy vs iPhone\n",
      "iPhone vs Samsung Galaxy\n",
      "iPhone vs Windows Phone\n",
      "Android vs iOS\n",
      "iPhone vs Chrome OS\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    bot.generate_response(\n",
    "        \"\"\"\n",
    "Human:\n",
    "Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.\n",
    "Assistant:\n",
    "Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\\n\\nI. Introduction\\nA. Overview of the topic\\nB. The purpose of the comparison\\nC. Background information on the two models\\n\\nII. Model Comparison\\nA. Key features and specifications\\nB. User reviews and ratings\\nC. Performance and battery life\\nD. Design and display quality\\nE. Price range and availability\\n\\nIII. Conclusion\\nA. Summary of the main points\\nB. Final thoughts and recommendations\\n\\nI hope this helps! Let me know if you have any other questions.\n",
    "Human:\n",
    "Take your previous response and rephrase it as a limerick.\n",
    "Assistant:\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:400 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'user', 'message': 'Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.'}, {'source': 'bot', 'message': 'Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\\n\\nI. Introduction\\nA. Overview of the topic\\nB. The purpose of the comparison\\nC. Background information on the two models\\n\\nII. Model Comparison\\nA. Key features and specifications\\nB. User reviews and ratings\\nC. Performance and battery life\\nD. Design and display quality\\nE. Price range and availability\\n\\nIII. Conclusion\\nA. Summary of the main points\\nB. Final thoughts and recommendations\\n\\nI hope this helps! Let me know if you have any other questions.'}, {'source': 'user', 'message': 'Take your previous response and rephrase it as a limerick.'}, {'source': 'bot', 'message': \"Sure! Here's my attempt at a limerick comparing two popular smartphones models:\\n\\niPhone vs Android\\nSamsung Galaxy vs iPhone\\niPhone vs Samsung Galaxy\\niPhone vs Windows Phone\\nAndroid vs iOS\\niPhone vs Chrome OS\"}, {'source': 'user', 'message': 'Привет, как дела?'}]\n",
      "\n",
      "Human:\n",
      "Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.\n",
      "Assistant:\n",
      "Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\n",
      "\n",
      "I. Introduction\n",
      "A. Overview of the topic\n",
      "B. The purpose of the comparison\n",
      "C. Background information on the two models\n",
      "\n",
      "II. Model Comparison\n",
      "A. Key features and specifications\n",
      "B. User reviews and ratings\n",
      "C. Performance and battery life\n",
      "D. Design and display quality\n",
      "E. Price range and availability\n",
      "\n",
      "III. Conclusion\n",
      "A. Summary of the main points\n",
      "B. Final thoughts and recommendations\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "Human:\n",
      "Take your previous response and rephrase it as a limerick.\n",
      "Assistant:\n",
      "Sure! Here's my attempt at a limerick comparing two popular smartphones models:\n",
      "\n",
      "iPhone vs Android\n",
      "Samsung Galaxy vs iPhone\n",
      "iPhone vs Samsung Galaxy\n",
      "iPhone vs Windows Phone\n",
      "Android vs iOS\n",
      "iPhone vs Chrome OS\n",
      "Human:\n",
      "Привет, как дела?\n",
      "Assistant:\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Здравствуйте'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat(\"Привет, как дела?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:400 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'bot', 'message': 'Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\\n\\nI. Introduction\\nA. Overview of the topic\\nB. The purpose of the comparison\\nC. Background information on the two models\\n\\nII. Model Comparison\\nA. Key features and specifications\\nB. User reviews and ratings\\nC. Performance and battery life\\nD. Design and display quality\\nE. Price range and availability\\n\\nIII. Conclusion\\nA. Summary of the main points\\nB. Final thoughts and recommendations\\n\\nI hope this helps! Let me know if you have any other questions.'}, {'source': 'user', 'message': 'Take your previous response and rephrase it as a limerick.'}, {'source': 'bot', 'message': \"Sure! Here's my attempt at a limerick comparing two popular smartphones models:\\n\\niPhone vs Android\\nSamsung Galaxy vs iPhone\\niPhone vs Samsung Galaxy\\niPhone vs Windows Phone\\nAndroid vs iOS\\niPhone vs Chrome OS\"}, {'source': 'user', 'message': 'Привет, как дела?'}, {'source': 'bot', 'message': 'Здравствуйте'}, {'source': 'user', 'message': 'Я готовлюсь ко сну'}]\n",
      "\n",
      "Assistant:\n",
      "Sure! Here is an outline for your comparison blog post comparing two popular smartphones models:\n",
      "\n",
      "I. Introduction\n",
      "A. Overview of the topic\n",
      "B. The purpose of the comparison\n",
      "C. Background information on the two models\n",
      "\n",
      "II. Model Comparison\n",
      "A. Key features and specifications\n",
      "B. User reviews and ratings\n",
      "C. Performance and battery life\n",
      "D. Design and display quality\n",
      "E. Price range and availability\n",
      "\n",
      "III. Conclusion\n",
      "A. Summary of the main points\n",
      "B. Final thoughts and recommendations\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "Human:\n",
      "Take your previous response and rephrase it as a limerick.\n",
      "Assistant:\n",
      "Sure! Here's my attempt at a limerick comparing two popular smartphones models:\n",
      "\n",
      "iPhone vs Android\n",
      "Samsung Galaxy vs iPhone\n",
      "iPhone vs Samsung Galaxy\n",
      "iPhone vs Windows Phone\n",
      "Android vs iOS\n",
      "iPhone vs Chrome OS\n",
      "Human:\n",
      "Привет, как дела?\n",
      "Assistant:\n",
      "Здравствуйте\n",
      "Human:\n",
      "Я готовлюсь ко сну\n",
      "Assistant:\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Хорошо'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat(\"Я готовлюсь ко сну\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 583/583 [00:00<00:00, 4.16MB/s]\n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 19.1MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [01:48<00:00, 92.1MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:38<00:00, 92.0MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [02:26<00:00, 73.42s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.71s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 179/179 [00:00<00:00, 1.38MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name in model.named_parameters():\n",
    "    print(name[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
