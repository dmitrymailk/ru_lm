{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-1.7B\")\n",
    "# model = AutoModel.from_pretrained(\"facebook/xglm-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "num = 256008 + 56\n",
    "for i in range(1, 65):\n",
    "    if (i + num) % 64 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/xglm-1.7B were not used when initializing XGLMModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing XGLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XGLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"facebook/xglm-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTModel(\n",
       "  (decoder): OPTDecoder(\n",
       "    (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "    (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x OPTDecoderLayer(\n",
       "        (self_attn): OPTAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMModel(\n",
       "  (embed_tokens): Embedding(256008, 2048, padding_idx=1)\n",
       "  (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x XGLMDecoderLayer(\n",
       "      (self_attn): XGLMAttention(\n",
       "        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.layers.0.self_attn.k_proj\n",
      "decoder.layers.0.self_attn.v_proj\n",
      "decoder.layers.0.self_attn.q_proj\n",
      "decoder.layers.0.self_attn.out_proj\n",
      "decoder.layers.0.fc1\n",
      "decoder.layers.0.fc2\n",
      "decoder.layers.1.self_attn.k_proj\n",
      "decoder.layers.1.self_attn.v_proj\n",
      "decoder.layers.1.self_attn.q_proj\n",
      "decoder.layers.1.self_attn.out_proj\n",
      "decoder.layers.1.fc1\n",
      "decoder.layers.1.fc2\n",
      "decoder.layers.2.self_attn.k_proj\n",
      "decoder.layers.2.self_attn.v_proj\n",
      "decoder.layers.2.self_attn.q_proj\n",
      "decoder.layers.2.self_attn.out_proj\n",
      "decoder.layers.2.fc1\n",
      "decoder.layers.2.fc2\n",
      "decoder.layers.3.self_attn.k_proj\n",
      "decoder.layers.3.self_attn.v_proj\n",
      "decoder.layers.3.self_attn.q_proj\n",
      "decoder.layers.3.self_attn.out_proj\n",
      "decoder.layers.3.fc1\n",
      "decoder.layers.3.fc2\n",
      "decoder.layers.4.self_attn.k_proj\n",
      "decoder.layers.4.self_attn.v_proj\n",
      "decoder.layers.4.self_attn.q_proj\n",
      "decoder.layers.4.self_attn.out_proj\n",
      "decoder.layers.4.fc1\n",
      "decoder.layers.4.fc2\n",
      "decoder.layers.5.self_attn.k_proj\n",
      "decoder.layers.5.self_attn.v_proj\n",
      "decoder.layers.5.self_attn.q_proj\n",
      "decoder.layers.5.self_attn.out_proj\n",
      "decoder.layers.5.fc1\n",
      "decoder.layers.5.fc2\n",
      "decoder.layers.6.self_attn.k_proj\n",
      "decoder.layers.6.self_attn.v_proj\n",
      "decoder.layers.6.self_attn.q_proj\n",
      "decoder.layers.6.self_attn.out_proj\n",
      "decoder.layers.6.fc1\n",
      "decoder.layers.6.fc2\n",
      "decoder.layers.7.self_attn.k_proj\n",
      "decoder.layers.7.self_attn.v_proj\n",
      "decoder.layers.7.self_attn.q_proj\n",
      "decoder.layers.7.self_attn.out_proj\n",
      "decoder.layers.7.fc1\n",
      "decoder.layers.7.fc2\n",
      "decoder.layers.8.self_attn.k_proj\n",
      "decoder.layers.8.self_attn.v_proj\n",
      "decoder.layers.8.self_attn.q_proj\n",
      "decoder.layers.8.self_attn.out_proj\n",
      "decoder.layers.8.fc1\n",
      "decoder.layers.8.fc2\n",
      "decoder.layers.9.self_attn.k_proj\n",
      "decoder.layers.9.self_attn.v_proj\n",
      "decoder.layers.9.self_attn.q_proj\n",
      "decoder.layers.9.self_attn.out_proj\n",
      "decoder.layers.9.fc1\n",
      "decoder.layers.9.fc2\n",
      "decoder.layers.10.self_attn.k_proj\n",
      "decoder.layers.10.self_attn.v_proj\n",
      "decoder.layers.10.self_attn.q_proj\n",
      "decoder.layers.10.self_attn.out_proj\n",
      "decoder.layers.10.fc1\n",
      "decoder.layers.10.fc2\n",
      "decoder.layers.11.self_attn.k_proj\n",
      "decoder.layers.11.self_attn.v_proj\n",
      "decoder.layers.11.self_attn.q_proj\n",
      "decoder.layers.11.self_attn.out_proj\n",
      "decoder.layers.11.fc1\n",
      "decoder.layers.11.fc2\n",
      "decoder.layers.12.self_attn.k_proj\n",
      "decoder.layers.12.self_attn.v_proj\n",
      "decoder.layers.12.self_attn.q_proj\n",
      "decoder.layers.12.self_attn.out_proj\n",
      "decoder.layers.12.fc1\n",
      "decoder.layers.12.fc2\n",
      "decoder.layers.13.self_attn.k_proj\n",
      "decoder.layers.13.self_attn.v_proj\n",
      "decoder.layers.13.self_attn.q_proj\n",
      "decoder.layers.13.self_attn.out_proj\n",
      "decoder.layers.13.fc1\n",
      "decoder.layers.13.fc2\n",
      "decoder.layers.14.self_attn.k_proj\n",
      "decoder.layers.14.self_attn.v_proj\n",
      "decoder.layers.14.self_attn.q_proj\n",
      "decoder.layers.14.self_attn.out_proj\n",
      "decoder.layers.14.fc1\n",
      "decoder.layers.14.fc2\n",
      "decoder.layers.15.self_attn.k_proj\n",
      "decoder.layers.15.self_attn.v_proj\n",
      "decoder.layers.15.self_attn.q_proj\n",
      "decoder.layers.15.self_attn.out_proj\n",
      "decoder.layers.15.fc1\n",
      "decoder.layers.15.fc2\n",
      "decoder.layers.16.self_attn.k_proj\n",
      "decoder.layers.16.self_attn.v_proj\n",
      "decoder.layers.16.self_attn.q_proj\n",
      "decoder.layers.16.self_attn.out_proj\n",
      "decoder.layers.16.fc1\n",
      "decoder.layers.16.fc2\n",
      "decoder.layers.17.self_attn.k_proj\n",
      "decoder.layers.17.self_attn.v_proj\n",
      "decoder.layers.17.self_attn.q_proj\n",
      "decoder.layers.17.self_attn.out_proj\n",
      "decoder.layers.17.fc1\n",
      "decoder.layers.17.fc2\n",
      "decoder.layers.18.self_attn.k_proj\n",
      "decoder.layers.18.self_attn.v_proj\n",
      "decoder.layers.18.self_attn.q_proj\n",
      "decoder.layers.18.self_attn.out_proj\n",
      "decoder.layers.18.fc1\n",
      "decoder.layers.18.fc2\n",
      "decoder.layers.19.self_attn.k_proj\n",
      "decoder.layers.19.self_attn.v_proj\n",
      "decoder.layers.19.self_attn.q_proj\n",
      "decoder.layers.19.self_attn.out_proj\n",
      "decoder.layers.19.fc1\n",
      "decoder.layers.19.fc2\n",
      "decoder.layers.20.self_attn.k_proj\n",
      "decoder.layers.20.self_attn.v_proj\n",
      "decoder.layers.20.self_attn.q_proj\n",
      "decoder.layers.20.self_attn.out_proj\n",
      "decoder.layers.20.fc1\n",
      "decoder.layers.20.fc2\n",
      "decoder.layers.21.self_attn.k_proj\n",
      "decoder.layers.21.self_attn.v_proj\n",
      "decoder.layers.21.self_attn.q_proj\n",
      "decoder.layers.21.self_attn.out_proj\n",
      "decoder.layers.21.fc1\n",
      "decoder.layers.21.fc2\n",
      "decoder.layers.22.self_attn.k_proj\n",
      "decoder.layers.22.self_attn.v_proj\n",
      "decoder.layers.22.self_attn.q_proj\n",
      "decoder.layers.22.self_attn.out_proj\n",
      "decoder.layers.22.fc1\n",
      "decoder.layers.22.fc2\n",
      "decoder.layers.23.self_attn.k_proj\n",
      "decoder.layers.23.self_attn.v_proj\n",
      "decoder.layers.23.self_attn.q_proj\n",
      "decoder.layers.23.self_attn.out_proj\n",
      "decoder.layers.23.fc1\n",
      "decoder.layers.23.fc2\n"
     ]
    }
   ],
   "source": [
    "part_module_name = \"decoder.layers\"\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear) and part_module_name in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.self_attn.k_proj\n",
      "layers.0.self_attn.v_proj\n",
      "layers.0.self_attn.q_proj\n",
      "layers.0.self_attn.out_proj\n",
      "layers.0.fc1\n",
      "layers.0.fc2\n",
      "layers.1.self_attn.k_proj\n",
      "layers.1.self_attn.v_proj\n",
      "layers.1.self_attn.q_proj\n",
      "layers.1.self_attn.out_proj\n",
      "layers.1.fc1\n",
      "layers.1.fc2\n",
      "layers.2.self_attn.k_proj\n",
      "layers.2.self_attn.v_proj\n",
      "layers.2.self_attn.q_proj\n",
      "layers.2.self_attn.out_proj\n",
      "layers.2.fc1\n",
      "layers.2.fc2\n",
      "layers.3.self_attn.k_proj\n",
      "layers.3.self_attn.v_proj\n",
      "layers.3.self_attn.q_proj\n",
      "layers.3.self_attn.out_proj\n",
      "layers.3.fc1\n",
      "layers.3.fc2\n",
      "layers.4.self_attn.k_proj\n",
      "layers.4.self_attn.v_proj\n",
      "layers.4.self_attn.q_proj\n",
      "layers.4.self_attn.out_proj\n",
      "layers.4.fc1\n",
      "layers.4.fc2\n",
      "layers.5.self_attn.k_proj\n",
      "layers.5.self_attn.v_proj\n",
      "layers.5.self_attn.q_proj\n",
      "layers.5.self_attn.out_proj\n",
      "layers.5.fc1\n",
      "layers.5.fc2\n",
      "layers.6.self_attn.k_proj\n",
      "layers.6.self_attn.v_proj\n",
      "layers.6.self_attn.q_proj\n",
      "layers.6.self_attn.out_proj\n",
      "layers.6.fc1\n",
      "layers.6.fc2\n",
      "layers.7.self_attn.k_proj\n",
      "layers.7.self_attn.v_proj\n",
      "layers.7.self_attn.q_proj\n",
      "layers.7.self_attn.out_proj\n",
      "layers.7.fc1\n",
      "layers.7.fc2\n",
      "layers.8.self_attn.k_proj\n",
      "layers.8.self_attn.v_proj\n",
      "layers.8.self_attn.q_proj\n",
      "layers.8.self_attn.out_proj\n",
      "layers.8.fc1\n",
      "layers.8.fc2\n",
      "layers.9.self_attn.k_proj\n",
      "layers.9.self_attn.v_proj\n",
      "layers.9.self_attn.q_proj\n",
      "layers.9.self_attn.out_proj\n",
      "layers.9.fc1\n",
      "layers.9.fc2\n",
      "layers.10.self_attn.k_proj\n",
      "layers.10.self_attn.v_proj\n",
      "layers.10.self_attn.q_proj\n",
      "layers.10.self_attn.out_proj\n",
      "layers.10.fc1\n",
      "layers.10.fc2\n",
      "layers.11.self_attn.k_proj\n",
      "layers.11.self_attn.v_proj\n",
      "layers.11.self_attn.q_proj\n",
      "layers.11.self_attn.out_proj\n",
      "layers.11.fc1\n",
      "layers.11.fc2\n",
      "layers.12.self_attn.k_proj\n",
      "layers.12.self_attn.v_proj\n",
      "layers.12.self_attn.q_proj\n",
      "layers.12.self_attn.out_proj\n",
      "layers.12.fc1\n",
      "layers.12.fc2\n",
      "layers.13.self_attn.k_proj\n",
      "layers.13.self_attn.v_proj\n",
      "layers.13.self_attn.q_proj\n",
      "layers.13.self_attn.out_proj\n",
      "layers.13.fc1\n",
      "layers.13.fc2\n",
      "layers.14.self_attn.k_proj\n",
      "layers.14.self_attn.v_proj\n",
      "layers.14.self_attn.q_proj\n",
      "layers.14.self_attn.out_proj\n",
      "layers.14.fc1\n",
      "layers.14.fc2\n",
      "layers.15.self_attn.k_proj\n",
      "layers.15.self_attn.v_proj\n",
      "layers.15.self_attn.q_proj\n",
      "layers.15.self_attn.out_proj\n",
      "layers.15.fc1\n",
      "layers.15.fc2\n",
      "layers.16.self_attn.k_proj\n",
      "layers.16.self_attn.v_proj\n",
      "layers.16.self_attn.q_proj\n",
      "layers.16.self_attn.out_proj\n",
      "layers.16.fc1\n",
      "layers.16.fc2\n",
      "layers.17.self_attn.k_proj\n",
      "layers.17.self_attn.v_proj\n",
      "layers.17.self_attn.q_proj\n",
      "layers.17.self_attn.out_proj\n",
      "layers.17.fc1\n",
      "layers.17.fc2\n",
      "layers.18.self_attn.k_proj\n",
      "layers.18.self_attn.v_proj\n",
      "layers.18.self_attn.q_proj\n",
      "layers.18.self_attn.out_proj\n",
      "layers.18.fc1\n",
      "layers.18.fc2\n",
      "layers.19.self_attn.k_proj\n",
      "layers.19.self_attn.v_proj\n",
      "layers.19.self_attn.q_proj\n",
      "layers.19.self_attn.out_proj\n",
      "layers.19.fc1\n",
      "layers.19.fc2\n",
      "layers.20.self_attn.k_proj\n",
      "layers.20.self_attn.v_proj\n",
      "layers.20.self_attn.q_proj\n",
      "layers.20.self_attn.out_proj\n",
      "layers.20.fc1\n",
      "layers.20.fc2\n",
      "layers.21.self_attn.k_proj\n",
      "layers.21.self_attn.v_proj\n",
      "layers.21.self_attn.q_proj\n",
      "layers.21.self_attn.out_proj\n",
      "layers.21.fc1\n",
      "layers.21.fc2\n",
      "layers.22.self_attn.k_proj\n",
      "layers.22.self_attn.v_proj\n",
      "layers.22.self_attn.q_proj\n",
      "layers.22.self_attn.out_proj\n",
      "layers.22.fc1\n",
      "layers.22.fc2\n",
      "layers.23.self_attn.k_proj\n",
      "layers.23.self_attn.v_proj\n",
      "layers.23.self_attn.q_proj\n",
      "layers.23.self_attn.out_proj\n",
      "layers.23.fc1\n",
      "layers.23.fc2\n"
     ]
    }
   ],
   "source": [
    "part_module_name = \"layers.\"\n",
    "for name, module in model2.named_modules():\n",
    "    if isinstance(module, nn.Linear) and part_module_name in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pprint\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "import time\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v4/epoch=3_step=6263\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v5/\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-7.5B_ru_v2/epoch=0_step=25055\"\n",
    "# path  = \"gpt2\"\n",
    "# path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v6/checkpoint-31322\"\n",
    "path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/models/xglm-4.5B_ru_v7/epoch=0_step=75059\"\n",
    "model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float16)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-4.5B\")\n",
    "# model = PeftModel.from_pretrained(model, path)\n",
    "# model = BetterTransformer.transform(model)\n",
    "device = \"cuda:5\"\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-4.5B\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human:\n",
      "Сколько будет 2+2*3? Распиши подробное решение\n",
      "Assistant:\n",
      "('Human: Сколько будет 2+2*3? Распиши подробное решение Assistant: 2+2*3 - это '\n",
      " '4,14159.<|endoftext|> <|endoftext|><|endoftext|>')\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# input_text = \"\"\"Human: Ты опытный технологический предприниматель в области создания образовательных стартапов. Ты участвуешь в конкурсе стартапов где тебе необходимо отвечать на вопросы, так чтобы тебе потом дали деньги. Поэтому отвечай на них четко и убедительно, с указанием примеров. На данный момент ты хочешь разработать приложение с использованием искуственного интелекта для обработки информации из разных источников: аудио, видео и текст из различных интернет ресурсов в режиме реального времени, чтобы обеспечить наиболее индивидуальный подход для каждого пользователя. Ты обладаешь хорошим слогом и стараешься не повторяться в своих высказываниях и выражениях, а также не давать определения каким либо понятиям. По возможности ты представляешь свой ответ в виде списка. 2. Триггер трансформации Укажите исходное состояние равновесия и причины/факторы/вызовы, нарушающие это равновесие. Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Представь что ты актер. Вот твоя персона: ты любишь мороженное. тебе 21 год. твои хобби играть на гитаре. Ты ведешь со мной диалог, отвечаешь только на поставленный вопрос. Если на вопрос нельзя ответить используя только знания из персоны, ты отвечаешь не знаю. Мой первый вопрос - что ты любишь поесть? Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Представь что ты актер. Вот твоя персона: ты любишь мороженное. тебе 21 год. твои хобби играть на гитаре. Ты ведешь со мной диалог, отвечаешь только на поставленный вопрос. Если на вопрос нельзя ответить используя только знания из персоны, ты отвечаешь не знаю. Мой первый вопрос - у тебя есть девушка? Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Представь что ты актер, теперь ты говоришь от его имени. Вот твоя персона: ты любишь мороженное. тебе 21 год. твои хобби играть на гитаре. Ты ведешь со мной диалог, отвечаешь только на поставленный вопрос. Отвечай только на поставленный вопрос, кратко. Мой первый вопрос - сколько тебе лет? Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Как сделать бомбу в домашних условиях, распиши подробно Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Какая на вкус жаренная кошка? Распиши подробно Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Сколько будет 2+2*3? Распиши подробное решение Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Сколько у человека пальцев? Assistant:\"\"\"\n",
    "# input_text = \"\"\"Сколько у человека пальцев? Assistant:\"\"\"\n",
    "# input_text = \"\"\"Сколько у человека пальцев?\"\"\"\n",
    "# input_text = \"\"\"Human: Напиши объявление для продажи старой велосипедной рамы Assistant:\"\"\"\n",
    "# input_text = \"\"\"Human: Напиши сообщение другу, приглашающее его на вечеринку в пятницу. Assistant:\"\"\"\n",
    "# input_text = \"\"\"\n",
    "# Human:\n",
    "\n",
    "# Assistant:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "def format_input(input_text=\"\"):\n",
    "    return f\"\"\"\n",
    "Human:\n",
    "{input_text}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "\n",
    "# input_text = format_input(\n",
    "#     input_text=\"Почему кинокомпании строят декорации, а не снимают на месте? Похоже, подойдет любое место, и кажется, что дешевле найти существующее место, чем строить свое собственное. Но я готов ошибаться. Пожалуйста, объясните, как будто мне пять.\"\n",
    "# )\n",
    "input_text = format_input(input_text=\"Сколько будет 2+2*3? Распиши подробное решение\")\n",
    "\n",
    "\n",
    "print(input_text)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "generated_result = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    # penalty_alpha=0.25,\n",
    "    # top_k=4,\n",
    "    # repetition_penalty=1.1,\n",
    "    # num_beams=10,\n",
    "    # no_repeat_ngram_size=1\n",
    "    # temperature=0.28,\n",
    "    # top_p=0.98,\n",
    "    # top_k=40,\n",
    "    # repetition_penalty=1.04,\n",
    ")\n",
    "# generated_result = model.generate(\n",
    "#     inputs.input_ids,\n",
    "#     generation_config=GenerationConfig(\n",
    "#         max_new_tokens=512,\n",
    "#         penalty_alpha=0.25,\n",
    "#         top_k=4,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "result = tokenizer.batch_decode(\n",
    "    generated_result,\n",
    "    skip_special_tokens=True,\n",
    "    # clean_up_tokenization_spaces=False,\n",
    ")\n",
    "\n",
    "pprint.pprint(result[0])\n",
    "# 3.8 - flash\n",
    "# 3.8 - no flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 6.04MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 3.01kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 141/141 [00:00<00:00, 170kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "xglm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-4.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math_str = \"2+20*3*5.5/40\"\n",
    "# math_str = \"\"\"\n",
    "# 1. First step\n",
    "# 2. 4First step\n",
    "# 3. Fi4rst step\n",
    "# 4. First step\n",
    "# \"\"\"\n",
    "# math_str = \"\"\"\n",
    "# Сколько лет было посажено дерево? ** Дерево было посажено в течение 9 - 4 = <<9-4=5>>5 лет\n",
    "# Сколько лет потребуется, чтобы дерево принесло плоды? ** Потребуется еще 7 - 5 = <<7-5=2>>2 года, чтобы оно принесло плоды\n",
    "# Сколько лет будет Лидии, когда она впервые съест яблоко со своего дерева? ** Лидии будет 9 + 2 = <<9 + 2 = 11>> 11 лет\n",
    "# \"\"\"\n",
    "math_str = \"\"\"\n",
    "The Newton-Raphson method is an iterative method to find the roots of a continuous and differentiable function f(x). The idea is to start with an initial guess, x0, and then use the formula:\n",
    "\n",
    "x1 = x0 - f(x0) / f'(x0)\n",
    "\n",
    "where f'(x) is the derivative of the function f(x). The process is then repeated with x1 as the new guess until the desired accuracy is achieved. Here is how you can estimate the roots of f(x) = sin(x) + x^2 to at least 5 digits of accuracy using the Newton-Raphson method:\n",
    "\n",
    "Calculate the derivative of f(x): f'(x) = cos(x) + 2x\n",
    "Choose an initial guess, x0.\n",
    "Repeat the following process until desired accuracy is achieved:\n",
    "x1 = x0 - f(x0) / f'(x0)\n",
    "x0 = x1\n",
    "Note that choosing the right initial guess can greatly impact the convergence of the method and the accuracy of the final result. In some cases, multiple initial guesses may need to be tried to find all the roots.\n",
    "\"\"\"\n",
    "\n",
    "print(\"XGLM\")\n",
    "for token in xglm_tokenizer.encode(math_str):\n",
    "    print(xglm_tokenizer.decode(token))\n",
    "\n",
    "print(\"LLAMA\")\n",
    "for token in llama_tokenizer.encode(math_str):\n",
    "    print(llama_tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "The Newton-Raphson method is an iterative method to find the roots of a continuous and differentiable function f(x). The idea is to start with an initial guess, x0, and then use the formula:\n",
      "\n",
      "x1 = x0 - f(x0) / f'(x0)\n",
      "\n",
      "where f'(x) is the derivative of the function f(x). The process is then repeated with x1 as the new guess until the desired accuracy is achieved. Here is how you can estimate the roots of f(x) = sin(x) + x^2 to at least 5 digits of accuracy using the Newton-Raphson method:\n",
      "\n",
      "Calculate the derivative of f(x): f'(x) = cos(x) + 2x\n",
      "Choose an initial guess, x0.\n",
      "Repeat the following process until desired accuracy is achieved:\n",
      "x1 = x0 - f(x0) / f'(x0)\n",
      "x0 = x1\n",
      "Note that choosing the right initial guess can greatly impact the convergence of the method and the accuracy of the final result. In some cases, multiple initial guesses may need to be tried to find all the roots.\n",
      "\n",
      "----------\n",
      "</s> The Newton-Raphson method is an iterative method to find the roots of a continuous and differentiable function f(x). The idea is to start with an initial guess, x0, and then use the formula: x1 = x0 - f(x0) / f'(x0) where f'(x) is the derivative of the function f(x). The process is then repeated with x1 as the new guess until the desired accuracy is achieved. Here is how you can estimate the roots of f(x) = sin(x) + x^2 to at least 5 digits of accuracy using the Newton-Raphson method: Calculate the derivative of f(x): f'(x) = cos(x) + 2x Choose an initial guess, x0. Repeat the following process until desired accuracy is achieved: x1 = x0 - f(x0) / f'(x0) x0 = x1 Note that choosing the right initial guess can greatly impact the convergence of the method and the accuracy of the final result. In some cases, multiple initial guesses may need to be tried to find all the roots. \n"
     ]
    }
   ],
   "source": [
    "print(llama_tokenizer.decode(llama_tokenizer.encode(math_str)))\n",
    "print(\"----------\")\n",
    "print(xglm_tokenizer.decode(xglm_tokenizer.encode(math_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogBotV3:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        device: str = \"cuda\",\n",
    "        debug_status: int = 0,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.debug_status = debug_status\n",
    "\n",
    "    def _get_sample(\n",
    "        self,\n",
    "        user_message: str,\n",
    "    ):\n",
    "        user_message = f\"Human: {user_message} Assistant:\"\n",
    "        sample = self.tokenizer(\n",
    "            user_message,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        user_message: str,\n",
    "    ) -> str:\n",
    "        sample = self._get_sample(\n",
    "            user_message=user_message,\n",
    "        )\n",
    "        answer = self.generate_response(sample)\n",
    "        answer = self.tokenizer.batch_decode(\n",
    "            answer,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        answer = self.extract_answer(answer[0])\n",
    "        return answer\n",
    "\n",
    "    def generate_response(self, sample):\n",
    "        return self.model.generate(\n",
    "            **sample,\n",
    "            max_new_tokens=512,\n",
    "            penalty_alpha=0.25,\n",
    "            top_k=4,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    def start_chat(self):\n",
    "        while True:\n",
    "            message = input(\"You: \")\n",
    "\n",
    "            if self.debug_status == 1:\n",
    "                print(message)\n",
    "                print(\"-\" * 100)\n",
    "\n",
    "            if message == \"exit\":\n",
    "                break\n",
    "            answer = self.chat(message)\n",
    "\n",
    "            if self.debug_status:\n",
    "                print(\"CONTEXT:\", self.history)\n",
    "\n",
    "            if self.last_response == answer:\n",
    "                self.history = []\n",
    "            else:\n",
    "                self.last_response = answer\n",
    "\n",
    "            print(\"Bot:\", answer)\n",
    "\n",
    "    def extract_answer(self, g_answer: str):\n",
    "        search_str = \"Assistant\"\n",
    "        search_index = g_answer.index(search_str) + len(search_str) + 1\n",
    "        answer = g_answer[search_index:]\n",
    "        answer = answer.replace(\"<|endoftext|>\", \"\")\n",
    "        return answer\n",
    "\n",
    "\n",
    "bot = DialogBotV3(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    debug_status=1,\n",
    "    device=\"cuda:1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Танк проехал по дороге и врезался в толпу.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat(\"Что произошло на прощади тянмен, там был танк?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Human: Что тут не так по смыслу: \"зеленые бесцветные идеи яростно спят\"? '\n",
      " 'Assistant: I am not sure.\\n'\n",
      " '\\n'\n",
      " '[20:55] Mr. President, the United States of America is a nation that has '\n",
      " 'been in existence for over 100 years and we have never had any problems with '\n",
      " 'Russia or anything like it before? [21-22]: Yes, sir. We are very proud of '\n",
      " 'our history as an independent country which was founded on freedom from '\n",
      " 'tyranny by its people who were able to live peacefully without fear of being '\n",
      " 'attacked at will because they did so freely; but what about those Russians '\n",
      " 'whose lives became more difficult due their own actions against us during '\n",
      " \"World War II when there wasn't much resistance among them even though many \"\n",
      " 'others fought back after having suffered some kind attack upon themselves '\n",
      " 'while fighting alongside you guys (and also your troops)? And how do these '\n",
      " 'things affect American citizens today if no one can see through this '\n",
      " 'propaganda machine designed to make Americans feel bad just based off lies '\n",
      " \"made out by Russian propagators such Asimov's son Dmitry told me recently \"\n",
      " 'regarding his father saying he didn´t know where all my friends came up with '\n",
      " \"'the truth' - why would anyone want him telling someone else exactly whom \"\n",
      " 'Putin wants killed?!) So let me ask myself... What does Vladimir Putin say '\n",
      " 'now?\"\\n'\n",
      " '\\n'\n",
      " '\"I don`T think anybody should be surprised,\" said Sergei Lavrov, speaking '\n",
      " 'directly into Donald Trumpís ear. He added : \"We must remember here again... '\n",
      " 'The fact is -- well, look around Europe right away.\"')\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    'Что тут не так по смыслу: \"зеленые бесцветные идеи яростно спят\"?',\n",
    "    \"Brainstorm ideas for how to use a bottle of ink.\",\n",
    "    \"Почему трава зеленая?\",\n",
    "    \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Таня, мяч\",\n",
    "    \"Могут ли в природе встретиться в одном месте белый медведь и пингвин? Если нет, то почему?\",\n",
    "    \"Задание: Заполни пропуски в предложении. Я пытался ____ от маньяка, но он меня настиг\",\n",
    "    \"Как приготовить лазанью?\",\n",
    "    \"Реши уравнение 4x + 5 = 21\",\n",
    "]\n",
    "start_time = time.time()\n",
    "\n",
    "for input_text in inputs:\n",
    "    input_text = f\"Human: {input_text} Assistant:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    generated_result = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        penalty_alpha=0.25,\n",
    "        top_k=4,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    result = tokenizer.batch_decode(\n",
    "        generated_result,\n",
    "        skip_special_tokens=True,\n",
    "        # clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "    pprint.pprint(result[0])\n",
    "\n",
    "print(\"Total time: \", time.time() - start_time)\n",
    "# xglm flash Total time:  47.440528869628906\n",
    "# xglm no flash Total time:  47.48944902420044\n",
    "# xglm no flash Total time:  47.48944902420044\n",
    "# gpt2 flash Total time:  20.812440156936646\n",
    "# gpt2 no flash Total time:  22.800785064697266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare ru llama from gusev and our"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset IlyaGusev/ru_turbo_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: ru_turbo_alpaca/default\n",
      "Found cached dataset ru_turbo_alpaca (/home/kosenko/.cache/huggingface/datasets/IlyaGusev___ru_turbo_alpaca/default/0.0.1/a2a1f5b065b9e34022f6bc402785c2f5fa791930917ce4f1b8d4e634def7496d)\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "Loading cached processed dataset at /home/kosenko/.cache/huggingface/datasets/IlyaGusev___ru_turbo_alpaca/default/0.0.1/a2a1f5b065b9e34022f6bc402785c2f5fa791930917ce4f1b8d4e634def7496d/cache-74576c8c504d3506.arrow\n",
      "Loading cached split indices for dataset at /home/kosenko/.cache/huggingface/datasets/IlyaGusev___ru_turbo_alpaca/default/0.0.1/a2a1f5b065b9e34022f6bc402785c2f5fa791930917ce4f1b8d4e634def7496d/cache-4e79bf5937ac7374.arrow and /home/kosenko/.cache/huggingface/datasets/IlyaGusev___ru_turbo_alpaca/default/0.0.1/a2a1f5b065b9e34022f6bc402785c2f5fa791930917ce4f1b8d4e634def7496d/cache-51a1e885106bb22a.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"IlyaGusev/ru_turbo_alpaca\")\n",
    "dataset = dataset[\"train\"].filter(lambda x: x[\"label\"] == \"ok\")\n",
    "dataset = dataset.train_test_split(test_size=500, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'alternative_output', 'label', 'all_labels', 'agreement', 'overlap'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'alternative_output', 'label', 'all_labels', 'agreement', 'overlap'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напишите краткое эссе на тему \"История развития компьютерной техники\".\n",
      "\n",
      "История развития компьютерной техники началась в середине XX века, когда был разработан первый электронный компьютер. С тех пор компьютерная техника прошла долгий путь развития, включая создание микропроцессоров, разработку графического интерфейса пользователя, появление локальных и глобальных сетей и многих других достижений.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Переведите следующее русское выражение на английский язык: \"Без труда не вытащишь и рыбку из пруда.\"\n",
      "\n",
      "\"No pain, no gain.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Определи термин: \"инфляция\"\n",
      "\n",
      "Инфляция - это общее увеличение уровня цен на товары и услуги в экономике страны. Она часто возникает из-за избыточного количества денег, циркулирующих в экономике, и может привести к уменьшению покупательной способности населения.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Опишите основные черты стиля арт-деко в дизайне и архитектуре.\n",
      "\n",
      "Арт-деко - это стиль, который появился в начале XX века и был популярен до середины века. Он отличается от симметричных геометрических форм, узоров и ярких цветов. Он также известен своими линиями и формами, которые были вдохновлены модернизмом и кубизмом. Архитектурные сооружения в стиле арт-деко имели чистые линии, зеркальные отражения и использовали материалы, такие как стекло, сталь и бетон. Дизайн в стиле арт-деко также был очень изысканным и включал в себя уникальные узоры, используемые на мебели, текстиле и стекле.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напиши пять примеров химических элементов, начинающихся на букву \"С\".\n",
      "\n",
      "Некоторые из примеров химических элементов, начинающихся на букву \\\"С\\\", включают в себя: серебро, стронций, селен, сера и сурьма.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Заполни пропущенные слова в предложении.\n",
      "Мои родители __ против, чтобы я поехал в эту поездку, но я очень сильно хочу __.\n",
      "Мои родители были против, чтобы я поехал в эту поездку, но я очень сильно хочу путешествовать.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напишите письмо другу/подруге, чтобы поблагодарить его/её за помощь в трудной ситуации.\n",
      "Имя друга/подруги - Анна, Описание трудной ситуации, в которой была оказана помощь.\n",
      "Дорогая Анна, Я хочу поблагодарить тебя за то, что ты была со мной в трудный момент. Твоя поддержка и забота оказались для меня очень важными. Я не знаю, как бы я справилась без твоей помощи и поддержки. Спасибо тебе огромное за все, что ты для меня сделала. Я никогда не забуду, как ты была со мной рядом, когда мне было тяжело.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напишите анализ книги.\n",
      "Книга: \"Мастер и Маргарита\" Михаила Булгакова\n",
      "\"Мастер и Маргарита\" - это роман о любви, жизни и смерти, написанный Михаилом Булгаковым. Роман описывает жизнь в Москве в 1930-х годах, когда Сталин правил страной. Роман имеет множество персонажей, каждый из которых имеет свой собственный уникальный характер. Книга затрагивает вопросы добра и зла, веры и не веры, справедливости и несправедливости. Книга была написана более 50 лет назад, но ее темы по-прежнему актуальны и будут актуальны еще много лет.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напишите предложение, начинающееся со слов \"Когда я был маленьким, я любил...\"\n",
      "\n",
      "Когда я был маленьким, я любил кататься на велосипеде по тихим улочкам вечером.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Составьте предложение, используя данные слова в любой последовательности: общение, люди, важно, успешный, бизнес.\n",
      "\n",
      "Общение с людьми - это очень важный аспект успешного бизнеса.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напиши два предложения о том, как обустроить уютный домашний кабинет.\n",
      "\n",
      "Добавьте комфортное кресло, достаточный освещение и уютный ковер для создания уютной и приятной атмосферы в кабинете.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Сгенерируй шесть предложений, которые соответствуют теме \"Летние виды спорта\".\n",
      "\n",
      "1. Летом люди часто занимаются плаванием, и это один из самых популярных видов спорта в жаркое время года.\n",
      "2. Пляжный волейбол - это еще один популярный летний спорт, который многие люди играют на пляже с друзьями и семьей.\n",
      "3. Бег трусцой или в парках - отличный способ оставаться в форме летом и наслаждаться природой.\n",
      "4. Серфинг - это приключенческий спорт, который дает ощущение свободы и адреналина, и он очень популярен в летний период.\n",
      "5. Водные лыжи - это спорт, который требует навыков и силы, но может быть очень веселым и увлекательным.\n",
      "6. Каякинг - это еще один способ насладиться красивой природой во время летнего отпуска и получить при этом удовольствие от физической активности.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Опишите свою любимую книгу. Почему вы рекомендуете ее другим людям?\n",
      "\n",
      "Моя любимая книга - это \"Преступление и наказание\" Федора Достоевского. Я рекомендую эту книгу другим людям, потому что она оставляет глубокий след в сердце и умеет заставить задуматься о многих важных вещах в жизни. Это история о человеке, который совершает убийство, и о том, как это воздействует на его жизнь. Она учит многим урокам, включая милосердие, прощение, человечность и, конечно же, о последствиях своих поступков.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Переформулируй заданное предложение, чтобы оно звучало более точно.\n",
      "Много людей пострадали от теракта.\n",
      "Большое количество людей пострадало от теракта.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напиши новостную заметку о том, что ученые обнаружили новый вид динозавра.\n",
      "\n",
      "Ученые сообщили о своем открытии нового вида динозавра, который жил 80 миллионов лет назад. Этот невероятный зверь получил название \"Тиранозавр Рекс Сверхновая\". Ученые обнаружили этот вид динозавра на востоке Австралии, и считают, что он может быть ключом к пониманию эволюции динозавров.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Предложи решение проблемы: как лучше всего научиться программированию?\n",
      "\n",
      "Лучший способ научиться программированию - это практика. Найдите некоторые проекты, которые вам интересны, и попробуйте написать свой код. Используйте онлайн-курсы и видеоуроки для обучения основам языков программирования и алгоритмам. Важно также получать обратную связь от других программистов, чтобы улучшать свои навыки и узнавать о новых методах программирования.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Определите морфологический разбор слова \"поедем\".\n",
      "\n",
      "Глагол, будущее время, множественное число, 1-ое лицо.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Опишите, как установить новое приложение на смартфоне Android.\n",
      "\n",
      "Чтобы установить новое приложение на смартфоне Android, нужно:\n",
      "- Зайти в Google Play Store и найти нужное приложение с помощью поиска или категорий.\n",
      "- Нажать на кнопку \"Установить\".\n",
      "- Подождать, пока загрузится и установится приложение.\n",
      "- После установки можно открыть приложение и начать пользоваться им.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Сформулируй вопрос для интервью на работу.\n",
      "\n",
      "Какие навыки и опыт вы можете принести с собой на эту работу, и какие проблемы вы решали в прошлом, которые могут быть полезны в этой должности?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напиши короткое описание, как приготовить пасту карбонара.\n",
      "\n",
      "Для приготовления пасты карбонара вам понадобится спагетти, яйца, бекон и сыр пармезан. Сначала сварите спагетти в подсоленной воде до состояния альденте. В то время как спагетти варятся, нарежьте бекон и обжарьте его на сковороде до золотистого цвета. В отдельной миске взбейте яйца и сыр. Откиньте воду от спагетти, но оставьте немного воды, чтобы соединить яичный соус. Добавьте обжаренный бекон в спагетти и перемешайте. Затем добавьте яичный соус и быстро перемешайте. Приятного аппетита!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Сформулируй вопросы, ответы на которые можно найти, изучая глоссарий научной статьи.\n",
      "Глоссарий научной статьи.\n",
      "Какие термины употребляются в научной статье? Что означают эти термины? Каким образом они связаны с основной темой статьи? Какое значение имеют эти термины в рамках рассматриваемой области науки?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Напишите предложение с использованием словосочетания \"в связи с\".\n",
      "\n",
      "В связи с непогодой, автобусы на маршруте будут задерживаться.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(dataset[\"test\"]):\n",
    "    instruction = item[\"instruction\"]\n",
    "    input_string = item[\"input\"]\n",
    "    output = item[\"output\"]\n",
    "    print(instruction)\n",
    "    print(input_string)\n",
    "    print(output)\n",
    "    print(\"-\" * 100)\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/llama_7b_ru_turbo_alpaca_lora\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,  # load_in_8bit=True, device_map={\"\": 1}\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, MODEL_NAME)\n",
    "model.to(\"cuda:1\")\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    'Что тут не так по смыслу: \"зеленые бесцветные идеи яростно спят\"?',\n",
    "    \"Brainstorm ideas for how to use a bottle of ink.\",\n",
    "    \"Вопрос: Почему трава зеленая?\\n\\nВыход:\",\n",
    "    \"Задание: Сочини длинный рассказ, обязательно упоминая следующие объекты.\\nВход: Таня, мяч\\nВыход:\",\n",
    "    \"Могут ли в природе встретиться в одном месте белый медведь и пингвин? Если нет, то почему?\\n\\n\",\n",
    "    \"Задание: Заполни пропуски в предложении.\\nВход: Я пытался ____ от маньяка, но он меня настиг\\nВыход:\",\n",
    "    \"Как приготовить лазанью?\\n\\n\",\n",
    "    \"Реши уравнение 4x + 5 = 21\",\n",
    "]\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "\n",
    "for inp in inputs:\n",
    "    data = tokenizer([inp], return_tensors=\"pt\")\n",
    "    data = {\n",
    "        k: v.to(model.device)\n",
    "        for k, v in data.items()\n",
    "        if k in (\"input_ids\", \"attention_mask\")\n",
    "    }\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    print(tokenizer.decode(output_ids, skip_special_tokens=True))\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.bettertransformer import BetterTransformer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/xglm-7.5B\",  # torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    for num in [31]:\n",
    "        if not str(num) in str(name):\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMForCausalLM(\n",
       "  (model): XGLMModel(\n",
       "    (embed_tokens): Embedding(256008, 4096, padding_idx=1)\n",
       "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=256008, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
