{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/xglm-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2, 113677,   3038]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(\"hello world\", return_tensors=\"pt\")\n",
    "model_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256008"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer) // 64 * 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n",
    "from flash_attn.bert_padding import unpad_input, pad_input\n",
    "\n",
    "\n",
    "# для того чтобы это заработало нужно открыть исходники и закоментировать все упоминания\n",
    "# TORCH_CHECK из сурсов, а затем скомпилировать это\n",
    "# искать в файле csrc/flash_attn/fmha_api.cpp\n",
    "def flash_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.Tensor] = None,\n",
    "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    **other_keys\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "    \"\"\"Input shape: Batch x Time x Channel\n",
    "\n",
    "    attention_mask: [bsz, q_len]\n",
    "    \"\"\"\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = (\n",
    "        self.q_proj(hidden_states)\n",
    "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    key_states = (\n",
    "        self.k_proj(hidden_states)\n",
    "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    value_states = (\n",
    "        self.v_proj(hidden_states)\n",
    "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        .transpose(1, 2)\n",
    "    )\n",
    "    # assert past_key_value is None, \"past_key_value is not supported\"\n",
    "    # assert not output_attentions, \"output_attentions is not supported\"\n",
    "    # assert not use_cache, \"use_cache is not supported\"\n",
    "\n",
    "    # Flash attention codes from\n",
    "    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n",
    "\n",
    "    # transform the data into the format required by flash attention\n",
    "    qkv = torch.stack(\n",
    "        [query_states, key_states, value_states], dim=2\n",
    "    )  # [bsz, nh, 3, q_len, hd]\n",
    "    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n",
    "    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n",
    "    # the attention_mask should be the same as the key_padding_mask\n",
    "    key_padding_mask = attention_mask\n",
    "\n",
    "    if key_padding_mask is None:\n",
    "        qkv = rearrange(qkv, \"b s ... -> (b s) ...\")\n",
    "        max_s = q_len\n",
    "        cu_q_lens = torch.arange(\n",
    "            0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32, device=qkv.device\n",
    "        )\n",
    "        output = flash_attn_unpadded_qkvpacked_func(\n",
    "            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n",
    "        )\n",
    "        output = rearrange(output, \"(b s) ... -> b s ...\", b=bsz)\n",
    "    else:\n",
    "        nheads = qkv.shape[-2]\n",
    "        x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n",
    "        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n",
    "        x_unpad = rearrange(\n",
    "            x_unpad,\n",
    "            \"nnz (three h d) -> nnz three h d\",\n",
    "            three=3,\n",
    "            h=nheads,\n",
    "        )\n",
    "        output_unpad = flash_attn_unpadded_qkvpacked_func(\n",
    "            x_unpad,\n",
    "            cu_q_lens,\n",
    "            max_s,\n",
    "            0.0,\n",
    "            softmax_scale=None,\n",
    "            causal=True,\n",
    "        )\n",
    "        output = rearrange(\n",
    "            pad_input(\n",
    "                rearrange(output_unpad, \"nnz h d -> nnz (h d)\"),\n",
    "                indices,\n",
    "                bsz,\n",
    "                q_len,\n",
    "            ),\n",
    "            \"b s (h d) -> b s h d\",\n",
    "            h=nheads,\n",
    "        )\n",
    "    return self.out_proj(rearrange(output, \"b s h d -> b s (h d)\")), None, None\n",
    "\n",
    "\n",
    "# Disable the transformation of the attention mask in LlamaModel as the flash attention\n",
    "# requires the attention mask to be the same as the key_padding_mask\n",
    "def _prepare_decoder_attention_mask(\n",
    "    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "):\n",
    "    # [bsz, seq_len]\n",
    "    return attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.models.xglm.modeling_xglm.XGLMAttention.forward = flash_forward\n",
    "transformers.models.xglm.modeling_xglm.XGLMModel._prepare_decoder_attention_mask = (\n",
    "    _prepare_decoder_attention_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPTXGLMAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return (\n",
    "            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        **other_parameters,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.dropout if self.training else 0,\n",
    "            is_causal=True,\n",
    "        )\n",
    "        y = self.resid_dropout(self.out_proj(y))\n",
    "        return y, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# !XGLM ATTENTION\n",
    "# def __init__(\n",
    "# self,\n",
    "# embed_dim: int,\n",
    "# num_heads: int,\n",
    "# dropout: float = 0.0,\n",
    "# is_decoder: bool = False,\n",
    "# bias: bool = True,\n",
    "# ):\n",
    "# super().__init__()\n",
    "# self.embed_dim = embed_dim\n",
    "# self.num_heads = num_heads\n",
    "# self.dropout = dropout\n",
    "# self.head_dim = embed_dim // num_heads\n",
    "\n",
    "# if (self.head_dim * num_heads) != self.embed_dim:\n",
    "#     raise ValueError(\n",
    "#         f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "#         f\" and `num_heads`: {num_heads}).\"\n",
    "#     )\n",
    "# self.scaling = self.head_dim**-0.5\n",
    "# self.is_decoder = is_decoder\n",
    "\n",
    "# self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "# self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "# self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "# self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "\n",
    "def nanoGPT_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "):\n",
    "    x = hidden_states\n",
    "    B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "    # efficient attention using Flash Attention CUDA kernels\n",
    "    y = torch.nn.functional.scaled_dot_product_attention(\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        attn_mask=None,\n",
    "        dropout_p=self.dropout if self.training else 0,\n",
    "        is_causal=True,\n",
    "    )\n",
    "    y = (\n",
    "        y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "    )  # re-assemble all head outputs side by side\n",
    "\n",
    "    # output projection\n",
    "    y = self.resid_dropout(self.c_proj(y))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.models.xglm.modeling_xglm.XGLMAttention = NanoGPTXGLMAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMForCausalLM(\n",
       "  (model): XGLMModel(\n",
       "    (embed_tokens): Embedding(260000, 2048)\n",
       "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x XGLMDecoderLayer(\n",
       "        (self_attn): NanoGPTXGLMAttention(\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=260000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XGLMForCausalLM\n",
    "\n",
    "model = XGLMForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model.resize_token_embeddings(len(tokenizer) // 64 * 65)\n",
    "model.half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss={'logits': tensor([[[-3.4326e-01, -4.6753e-01,  1.0950e+02,  ..., -1.0352e+00,\n",
       "          -3.9038e-01,  6.0742e-01],\n",
       "         [ 6.3818e-01,  8.8623e-01,  1.3825e+02,  ..., -1.2451e+00,\n",
       "          -4.8169e-01,  7.6660e-01],\n",
       "         [ 5.0859e+00,  4.7422e+00,  2.1538e+02,  ..., -1.4395e+00,\n",
       "          -7.0435e-02, -6.3538e-02]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), 'past_key_values': (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)}, logits=tensor([[[-3.4326e-01, -4.6753e-01,  1.0950e+02,  ..., -1.0352e+00,\n",
       "          -3.9038e-01,  6.0742e-01],\n",
       "         [ 6.3818e-01,  8.8623e-01,  1.3825e+02,  ..., -1.2451e+00,\n",
       "          -4.8169e-01,  7.6660e-01],\n",
       "         [ 5.0859e+00,  4.7422e+00,  2.1538e+02,  ..., -1.4395e+00,\n",
       "          -7.0435e-02, -6.3538e-02]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), past_key_values=(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**model_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
