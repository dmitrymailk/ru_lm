{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "class GoralConversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=\" <s> {role}\\n{content} </s>\\n\",\n",
    "        system_prompt=\"Ты — Горал, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\",\n",
    "        start_token_id=1,\n",
    "        bot_token_id=9225,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.start_token_id = start_token_id\n",
    "        self.bot_token_id = bot_token_id\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def get_start_token_id(self):\n",
    "        return self.start_token_id\n",
    "\n",
    "    def get_bot_token_id(self):\n",
    "        return self.bot_token_id\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += tokenizer.decode(\n",
    "            [\n",
    "                self.start_token_id,\n",
    "            ]\n",
    "        )\n",
    "        final_text += \" \"\n",
    "        final_text += tokenizer.decode([self.bot_token_id])\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "# weights_path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/rulm/rulm2/rulm/self_instruct/models/saiga2_13b_v4\"\n",
    "weights_path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/rulm/rulm2/rulm/self_instruct/models/saiga2_v2\"\n",
    "# weights_path = \"dim/llama2_13b_dolly_oasst1_chip2\"\n",
    "access_token = \"\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    token=access_token,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(weights_path)\n",
    "generation_config = GenerationConfig.from_pretrained(weights_path)\n",
    "generation_config.do_sample = False\n",
    "\n",
    "\n",
    "inp = \"Напишите интересный пост в блоге о недавней поездке на Гавайи, рассказывая о культурном опыте и достопримечательностях, которые обязательно нужно увидеть.\"\n",
    "conversation = GoralConversation(\n",
    "    start_token_id=1,\n",
    "    bot_token_id=9225,\n",
    ")\n",
    "conversation.add_user_message(inp)\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4d4d3cc9114e21b1c30d8442a0cf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dim/llama2_7b_dolly_oasst1_chip2/commit/f3d716e818c402fa37fe177fc18c938d8c12a7f5', commit_message='Upload model', commit_description='', oid='f3d716e818c402fa37fe177fc18c938d8c12a7f5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.push_to_hub(\"dim/llama2_13b_dolly_oasst1_chip2\")\n",
    "# model.push_to_hub(\"dim/llama2_7b_dolly_oasst1_chip2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/huggingface_hub/utils/_hf_folder.py:95: UserWarning: A token has been found in `/home/kosenko/.huggingface/token`. This is the old path where tokens were stored. The new location is `/home/kosenko/.cache/huggingface/token` which is configurable using `HF_HOME` environment variable. Your token has been copied to this new location. You can now safely delete the old token file manually or use `huggingface-cli logout`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcf50b12e0f43ebb2646b1c491d3152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5234bce3404396ba844bec4bb23618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XGLMForCausalLM were not initialized from the model checkpoint at facebook/xglm-4.5B and are newly initialized: ['model.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf70a3af7ab9448f9f3829535152613e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напишите интересный пост в блоге о недавней поездке на Гавайи, рассказывая о культурном опыте и достопримечательностях, которые обязательно нужно увидеть.\n",
      "Я был там! Это было незабываемое путешествие, которое я никогда не забуду. Мы посетили все основные достопримечательности острова, включая пляжи, вулканы, пещеры, национальные парки и многое другое. Впечатления от посещения были потрясающими, а культура - уникальной. Поездка была отличным способом исследовать остров и узнать больше об истории его жителей. Надеюсь, что вы также захотите посетить это место!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "class GoralConversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=\" <s> {role}\\n{content} </s>\\n\",\n",
    "        system_prompt=\"Ты — Горал, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\",\n",
    "        start_token_id=1,\n",
    "        bot_token_id=9225,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.start_token_id = start_token_id\n",
    "        self.bot_token_id = bot_token_id\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def get_start_token_id(self):\n",
    "        return self.start_token_id\n",
    "\n",
    "    def get_bot_token_id(self):\n",
    "        return self.bot_token_id\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += tokenizer.decode(\n",
    "            [\n",
    "                self.start_token_id,\n",
    "            ]\n",
    "        )\n",
    "        final_text += \" \"\n",
    "        final_text += tokenizer.decode([self.bot_token_id])\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "weights_path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/rulm/self_instruct/models/goral_xglm_4.5B\"\n",
    "access_token = \"\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    token=access_token,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(weights_path)\n",
    "generation_config = GenerationConfig.from_pretrained(weights_path)\n",
    "generation_config.do_sample = False\n",
    "\n",
    "\n",
    "inp = \"Напишите интересный пост в блоге о недавней поездке на Гавайи, рассказывая о культурном опыте и достопримечательностях, которые обязательно нужно увидеть.\"\n",
    "conversation = GoralConversation(\n",
    "    start_token_id=0,\n",
    "    bot_token_id=7425,\n",
    ")\n",
    "conversation.add_user_message(inp)\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "print(inp)\n",
    "print(output)\n",
    "# Я был там! Это было незабываемое путешествие, которое я никогда не забуду. Мы посетили все основные достопримечательности острова, включая пляжи, вулканы, пещеры, национальные парки и многое другое. Впечатления от посещения были потрясающими, а культура - уникальной. Поездка была отличным способом исследовать остров и узнать больше об истории его жителей. Надеюсь, что вы также захотите посетить это место!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d3207b690b4c248519918749f685cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/50.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dim/xglm-4.5b_dolly_oasst1_chip2/commit/8672de00f8ed2402047ed1380cc9e7fb3163d7d8', commit_message='Upload model', commit_description='', oid='8672de00f8ed2402047ed1380cc9e7fb3163d7d8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.push_to_hub(\"dim/xglm-4.5b_dolly_oasst1_chip2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RU-GPT-13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "class GoralConversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=\" <s> {role}\\n{content} </s>\\n\",\n",
    "        system_prompt=\"Ты — Горал, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\",\n",
    "        start_token_id=1,\n",
    "        bot_token_id=9225,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.start_token_id = start_token_id\n",
    "        self.bot_token_id = bot_token_id\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def get_start_token_id(self):\n",
    "        return self.start_token_id\n",
    "\n",
    "    def get_bot_token_id(self):\n",
    "        return self.bot_token_id\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += tokenizer.decode(\n",
    "            [\n",
    "                self.start_token_id,\n",
    "            ]\n",
    "        )\n",
    "        final_text += \" \"\n",
    "        final_text += tokenizer.decode([self.bot_token_id])\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "weights_path = \"/home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/rulm/self_instruct/models/rugpt_v1\"\n",
    "access_token = \"\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    token=access_token,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(weights_path)\n",
    "generation_config = GenerationConfig.from_pretrained(weights_path)\n",
    "generation_config.do_sample = False\n",
    "\n",
    "\n",
    "inp = \"Напишите интересный пост в блоге о недавней поездке на Гавайи, рассказывая о культурном опыте и достопримечательностях, которые обязательно нужно увидеть.\"\n",
    "conversation = GoralConversation(\n",
    "    start_token_id=2,\n",
    "    bot_token_id=46787,\n",
    ")\n",
    "conversation.add_user_message(inp)\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "print(inp)\n",
    "print(output)\n",
    "# Гавайские острова - это рай для любителей природы, культуры и приключений. Это место, где можно исследовать тропические леса, плавать с дельфинами, кататься на волнах прибоя или просто наслаждаться солнцем и пляжем. В дополнение к этому, здесь есть множество достопримечательностей, которые стоит посетить, таких как вулканы Мауна-Лоа и Килауэа, водопады Ваймеа, пещеры Халеакала и многое другое. Если вы ищете что-то более расслабляющее, то посещение гавайских ресторанов и баров может быть отличным способом провести время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"dim/ruGPT-13b_dolly_oasst1_chip2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
