{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from qlora_train import create_prompt_dataset_v2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/chip2_instruct_alpha_prompt_en_v2_clean_v1/cache-1c4c7215bfe16982.arrow and /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/chip2_instruct_alpha_prompt_en_v2_clean_v1/cache-54085008405851ef.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname 00430737cbd813b41bcf80abc44368ad9ac5133372d1292b5c2ce034188cdabd\n",
      "cache_found False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/chip2_instruct_alpha_prompt_ru_v2_clean_v1/cache-c584f28b24e37d96.arrow and /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/chip2_instruct_alpha_prompt_ru_v2_clean_v1/cache-b1db1d1113bc4791.arrow\n",
      "Loading cached split indices for dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/dolly_original_prompt_v2/cache-309ac6eb7efff8cc.arrow and /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/dolly_original_prompt_v2/cache-93d5e78eeef1a509.arrow\n",
      "Loading cached split indices for dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/dolly_translated_prompt_v2_clean_v1/cache-3f1ddd9a7fedc4c7.arrow and /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/dolly_translated_prompt_v2_clean_v1/cache-d4163d9a1d3394fd.arrow\n",
      "Loading cached split indices for dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_en_v2_clean_v1/cache-d58fb1a8c4bdbf07.arrow and /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_en_v2_clean_v1/cache-9481c176b0a8c0bb.arrow\n",
      "Loading cached split indices for dataset at /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_ru_v2_clean_v1/cache-0336d2856b149137.arrow and /home/kosenko/deepspeed/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/datasets/prompt_datasets/openass_prompt_dataset_ru_v2_clean_v1/cache-7f0e00243a0508df.arrow\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37572527983423637"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"togethercomputer/RedPajama-INCITE-7B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer._pad_token is None:\n",
    "    DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "    special_tokens_dict = dict(pad_token=DEFAULT_PAD_TOKEN)\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "datasets = \"chip2_instruct_alpha_prompt_en_v2_clean_v1 chip2_instruct_alpha_prompt_ru_v2_clean_v1 dolly_original_prompt_v2 dolly_translated_prompt_v2_clean_v1 openass_prompt_dataset_en_v2_clean_v1 openass_prompt_dataset_ru_v2_clean_v1\".split()\n",
    "\n",
    "train, valid = create_prompt_dataset_v2(\n",
    "    datasets_names=datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=2048,\n",
    "    output_path=\"./datasets/\",\n",
    "    seed=1234,\n",
    ")\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(1234)\n",
    "train_dataset = train\n",
    "sampler = RandomSampler(train_dataset, generator=generator)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    max_length=2048,\n",
    ")\n",
    "\n",
    "\n",
    "def collator(x):\n",
    "    features_map = data_collator(x)\n",
    "    return features_map\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset, batch_size=1, sampler=sampler, collate_fn=collator\n",
    ")\n",
    "\n",
    "\n",
    "# the length of the dataset in tokens\n",
    "L_t = 0\n",
    "#  the length of the dataset in UTF-8 encoded bytes.\n",
    "L_b = 0\n",
    "# cross entropy loss\n",
    "loss = 1.095\n",
    "\n",
    "for i, item in zip(range(20000), data_loader):\n",
    "    # print(item)\n",
    "    tokens = item[\"input_ids\"][0]\n",
    "    L_t += len(tokens)\n",
    "    decoded_str = tokenizer.decode(tokens)\n",
    "    L_b += len(decoded_str)\n",
    "    # print(decoded_str)\n",
    "\n",
    "\n",
    "def tokens_per_byte(\n",
    "    L_t=None,\n",
    "    L_b=None,\n",
    "    loss=None,\n",
    "):\n",
    "    return (L_t / L_b) * loss / math.log2(2)\n",
    "\n",
    "\n",
    "tokens_per_byte(L_t=L_t, L_b=L_b, loss=loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
